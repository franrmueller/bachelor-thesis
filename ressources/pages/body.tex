\section{Introduction}\label{sec:intro}
Organizations across industries continue to face persistent challenges in achieving operational excellence (OpEx). Fragmented processes, manual interventions, and inconsistent data quality undermine efficiency and decision-making. Legacy workflows and siloed systems exacerbate these inefficiencies, while traditional automation approaches often lack the adaptability needed in dynamic business environments. For companies, this translates into slower response times, higher compliance risks, and limited scalability—issues that directly threaten competitiveness.

Agentic AI, building on the advances of generative artificial intelligence (GenAI), opens new possibilities to extend automation beyond deterministic scripts. While GenAI provides the cognitive and generative capabilities, agentic AI leverages these to create adaptive, tool-using agents that can plan, act, and coordinate—thereby supporting governance, decision quality, and organizational agility. Despite this potential, both practice and academic literature lack structured strategies and conceptual frameworks for embedding such agentic capabilities into operational workflows in a scalable and value-driven way. This gap motivates the present research.

In this context, multi-agent systems can serve as a reference architecture for integrating GenAI-enabled agentic AI into enterprise workflow automation. The central research question is:

\vspace{0.5\baselineskip}
\emph{How can a multi-agent architecture be designed to integrate GenAI capabilities into workflow automation, in order to enhance agility, compliance, \& decision quality to achieve OpEx?}
\vspace{0.5\baselineskip}

To answer this question, the study addresses the following sub-questions:
\begin{itemize}
    \item \emph{Which design requirements \& agent roles are necessary to align a multi-agent architecture with the goals of OpEx?}
    \item \emph{How should a multi-agent system be architected to fulfill these requirements?}
    \item \emph{Under which conditions is deploying a generative multi-agent architecture justified over traditional automation approaches?}
\end{itemize}

Methodologically, the thesis applies Design Science Research (DSR) to develop a conceptual reference architecture. The approach synthesizes requirements from academic literature and OpEx principles, models agent roles and interactions, and derives applicability conditions for real-world deployment.
Although the architecture is designed to remain industry-agnostic, a use case from the financial services sector is introduced to illustrate how the conceptual model can be instantiated in a regulated, legacy-intensive environment.

The core contribution of this work is a conceptual design of a multi-agent system that leverages GenAI to support OpEx in enterprise workflows. Specifically, it delivers:
\begin{itemize}
    \item \emph{A structured synthesis of system requirements derived from academic literature and OpEx principles.}
    \item \emph{A conceptual architecture detailing agent roles, interactions, and integration points.}
    \item \emph{A set of applicability conditions and design considerations to guide future deployment and evaluation of generative multi-agent architectures in practice.}
\end{itemize}

The scope is limited to conceptual design; formal evaluation and technical implementation are proposed as future work. Although the architecture is designed to remain industry-agnostic, a use case from the financial services sector is introduced to illustrate how the conceptual model can be instantiated in a regulated, legacy-intensive environment.

The thesis is structured as follows: Section 2 outlines the research methodology, including DSR and supporting methods. Section 3 presents a literature review on OpEx, automation paradigms, and multi-agent systems. Section 4 develops applicability conditions and use case illustrations. Section 5 introduces the conceptual architecture design, and Section 6 concludes with reflections and directions for future research.

\section{Methodology}\label{sec:method}
This thesis applies design science research methodology to create a conceptual artifact---a multi-agent architecture for workflow automation. Practically, the approach unfolded in three steps: (1) \emph{reviewing the literature} on OpEx, workflow automation, and agentic AI;~(2) \emph{deriving and structuring requirements} from literature and case material into a requirements model; and (3) \emph{designing a conceptual system architecture} using System Modeling Language (SysML).

Supporting methods included Mayring-style qualitative content analysis (QCA) for the review, requirements engineering (RE) and systems analysis for the requirements model, and information systems design (ISD) to structure the architecture and ensure requirement-to-design traceability, supported by SysML modeling practices from Model-Based Systems Engineering (MBSE). Within DSR, the work focuses on problem identification, objective definition, and conceptual design, while instantiation/demonstration and formal evaluation are out of scope given the bachelor-thesis format and resource constraints. This scoping maintains methodological rigor while keeping the contribution focused: a well-argued reference architecture ready for subsequent implementation and empirical evaluation.

\subsection{Qualitative Content Analysis}\label{subsec:qca}
To ensure a structured literature review, this thesis employed qualitative content analysis following \textcite{mayringQualitative2022}. QCA offers a transparent, rule-based procedure for synthesizing knowledge from textual sources while retaining interpretative depth. In this work it supports the DSR process \parencite{peffersDesign2007} within the problem identification and motivation phase, where the aim is to understand the state of the problem domain and justify the value of a solution. Following Mayring, the analytical framework was defined prior to coding:
\begin{itemize}
    \item \textit{Analysis unit}: the overall literature corpus addressing OpEx, workflow automation, and agentic AI.\@
    \item \textit{Context unit}: individual publications (books, peer-reviewed articles, standards, industry reports, conference transcripts, and case studies).
    \item \textit{Coding unit}: discrete statements or conceptual claims relevant to the intersection of OpEx, automation paradigms, and AI-based multi-agent systems.
\end{itemize}

A \emph{mixed deductive-inductive} approach was used. Deductive categories were derived from established theory, including OpEx dimensions (adaptability, compliance, decision quality) and prior automation frameworks (robot process automation, RPA;~intelligent process automation, IPA). Inductive categories were generated from the material itself, capturing emerging issues such as ``guardrails,'' ``observability,'' and ``traceability'' in agentic AI systems.  Coding followed Mayring's rule-governed categorization to ensure consistency and avoid arbitrary interpretation. The resulting categories served two functions:
\begin{itemize}
    \item \textit{Problem representation}: categories structured how the research problem was represented, aligning with \textcite{hevnerDesign2004}, who emphasize that effective constructs are essential to problem framing.
    \item \textit{Derivation of objectives}: categories were transformed into metarequirements that guided the definition of solution objectives in Activity~2 of the DSR methodology \parencite{peffersDesign2007}.
\end{itemize}

The review was conducted by systematically coding the literature across the three pillars (OpEx, workflow automation, multi-agent systems). 
For example, claims such as ``RPA is brittle under interface changes'' were coded under the deductive category \emph{limitations of RPA}, 
while repeated references to audit trails and logging practices were inductively grouped under \emph{traceability}. 
For each publication, relevant statements were assigned to categories using predefined coding rules. 
The resulting category set both structures Section~\ref{sec:lit-rev} and forms the basis for the requirements engineering in Section~\ref{subsec:re-sa}. Additional categories such as efficiency, customer-centricity, and user empowerment emerged inductively during coding; these are elaborated in Section~\ref{subsec:op-ex}.

\subsection{Requirements Engineering \& Systems Analysis}\label{subsec:re-sa}
Within the DSR methodology, clearly articulating the problem and objectives (i.e.~meta-requirements) is essential before artifact design \parencite{hevnerDesign2004,peffersDesign2007}. Following the \textcite{IEEEStandard1990} definition, a requirement is a \emph{condition or capability needed by a user to solve a problem or achieve an objective}. Requirements Engineering (RE) provides the systematic means to derive such objectives. In this thesis, RE was deliberately applied in the early phases to ensure that the conceptual architecture rests on precise, validated needs rather than general aspirations.

Consistent with \textcite{herrmannGrundlagen2022}, the RE process was conducted iteratively through elicitation, documentation, analysis, and management. Requirements were drawn from the QCA results of the literature review and case context, then specified and validated for completeness and consistency. Using the \textcite{glinzHandbook2020} taxonomy, they were classified into \emph{functional requirements} (system behaviors), \emph{quality requirements} (non-functional attributes such as performance or compliance), and \emph{constraints} (technological or regulatory limits). To situate these requirements, a complementary systems analysis defined the system boundary, identified stakeholders and external actors, and clarified interface obligations—helping prevent scope creep and omissions \parencite{herrmannGrundlagen2022}.

The requirements were then represented in SysML requirement diagrams. Trace links connect each documented requirement to the respective architecture elements, enabling full requirement-to-design traceability. This model-based approach ensures that design decisions can always be traced back to validated needs and that verification can later check no requirement was overlooked.

To assess the *quality* of the requirement set, the concept of meta-requirements was introduced. According to \textcite{kaiyaMeta2018}, well-formed system requirements generally (1) reduce human effort, (2) increase human gain, (3) support sustainability, and (4) foster user participation. These served as high-level validation criteria. For example, requirements targeting the automation of routine steps align with reducing effort, while those improving decision accuracy reflect increased gain. Sustainability and participation considerations further ensured that requirements support long-term viability and inclusive use. This meta-level review safeguarded that requirements not only address immediate needs but also reflect broader design principles.

In summary, the integration of RE and systems analysis provided a structured, traceable, and quality-assured requirement set. This foundation anchors the subsequent conceptual architecture in rigorously defined objectives, ensuring consistency with both DSR methodology and operational excellence goals.

\subsection{Information Systems Design}\label{subsec:isd}
In line with DSR, this thesis applies principles of information systems design to structure the artifact. The architecture was modeled in SysML, making use of MBSE practices to ensure requirement-to-design traceability. While originating in MBSE originates in systems engineering, its modeling discipline is transferable to information systems contexts and supports the systematic development of conceptual architectures.

A conceptual architecture was systematically developed based on the previously synthesized requirements using a MBSE approach. MBSE provides a formalized way to transform requirements into a rigorous system model and to validate the design at the conceptual level. In fact, MBSE is defined as the formal application of modeling to support system requirements, design, and analysis, along with verification and validation, beginning in the conceptual design phase. Adopting MBSE thus ensured that even at this early stage, the architecture could be checked against stakeholder needs and constraints. The methodology enabled a unified representation of the entire system that made it possible to study interactions among components and agents before implementation. Each requirement was traced to corresponding elements in the model (e.g.~agent roles, interactions, or policies), guaranteeing requirements-to-design traceability and consistency throughout the design process.

The conceptual architecture was captured as a SysML v2 model, using the latest OMG Systems Modeling Language standard for systems engineering GitHub. All modeling was performed with the open-source OMG reference implementation of SysMLv2 on GitHub, leveraging a plain-text modeling workflow in Visual Studio Code. This setup allowed the use of SysMLv2's textual notation to define the system's structure and behavior in a tool-agnostic, version-controlled manner. The choice of SysMLv2 (over SysMLv1 or informal diagrams) is justified by its improved expressiveness and alignment with current MBSE best practices; as an OMG-developed language it provides robust semantics for specifying complex interactions in multi-agent systems while remaining an emerging industry standard. In summary, the information system's design was conducted in a model-driven fashion: the SysML~v2 conceptual model served as an executable blueprint of the architecture, facilitating early validation of design decisions against the requirements and providing a solid foundation for subsequent development steps (cf. Madni 2023; OMG 2023).

\section{Literature Review}\label{sec:lit-rev}

\subsection{Operational Excellence}\label{subsec:op-ex}
Operational excellence originated as a management philosophy in the manufacturing sector, particularly in the automotive industry to optimize quality and efficiency (Lean, Six Sigma, and Total Quality Management). In this classical context, OpEx focused on minimizing defects, eliminating waste, and embedding continuous improvement practices into organizational routines \parencite{juranQuality1999, womackLean2013}. While these roots remain important, they provide only a partial foundation for understanding OpEx in today's IT-driven enterprises, which operate in volatile environments shaped by rapid technological change, regulatory complexity, and global competition.

In IT-driven firms, OpEx is defined less by physical production flows and more by the ability to execute business strategies effectively and efficiently while maintaining innovation and adaptability. A systematic review by \textcite{owoadeSystematic2024} emphasizes that strategic decision-making, leadership, process optimization, and technology integration are the principal drivers of operational excellence in IT.~Leaders align strategic goals with day-to-day operations, process optimization ensures efficiency and service reliability, and the integration of emerging technologies---such as cloud computing, automation, and artificial intelligence---enables scalability, agility, and data-driven decision-making. These elements together form the foundation for delivering operational performance in a digital economy.

Recent research points to a structural tension between the stability fostered by OpEx and the adaptability required by organizational agility. According to \textcite{carvalhoOperational2023}, OpEx programs benefit organizations operating in stable contexts by promoting efficiency and rigor, but in volatile environments these same characteristics may reduce responsiveness and even become counterproductive. Agility, in contrast, emphasizes change-readiness and rapid adaptation, qualities increasingly seen as indicators of organizational excellence in globalized and unpredictable markets. The trade-offs are visible: process maturity can limit flexibility, while a focus on speed may undermine quality or compliance. Firms therefore face the challenge of reconciling continuous improvement with adaptability.

However, IT firms also face significant challenges: resistance to change among employees, misalignment between strategy and operational execution, and compliance with evolving regulations.~Externally, competition and technological disruption add further pressures, requiring firms to balance efficiency with innovation. Internally, communication breakdowns and shortages of skilled personnel can limit the adoption of excellence programs. Overcoming these barriers requires strong leadership commitment, alignment of culture with strategic goals, and sustained investment in skills and infrastructure.

From these insights, operational excellence in IT can be understood as a dynamic balance: stability through disciplined continuous improvement, complemented by agility to adapt under uncertainty. For mature IT organizations, OpEx provides the governance and process reliability needed to scale, while agility ensures responsiveness to change. For startups, OpEx offers a stabilizing framework to institutionalize quality, upon which agile practices can later be layered. This duality directly informs workflow automation in IT:~automation systems must simultaneously enforce compliance and quality while enabling rapid adaptation and innovation.

Applying Mayring's QCA approach, the analysis of OpEx literature yields the following categories relevant to IT-driven workflow automation:
\begin{itemize}
    \item \textsc{adaptability and agility:} the capacity to reconfigure processes in response to volatility.
    \item \textsc{compliance and risk management:} embedding regulatory adherence and transparency into workflows.
    \item \textsc{decision quality:} fostering data-driven and timely decision-making through analytics.
    \item \textsc{efficiency and continuous improvement:} reducing waste, automating repetitive tasks, and institutionalizing iterative refinements.
    \item \textsc{customer-centricity:} aligning operations with user needs and service-level commitments.
    \item \textsc{user empowerment and culture:} supporting collaboration, transparency, and employee engagement in improvement processes.
    \item \textsc{technology integration and scalability:} ensuring architectures can incorporate automation, AI, and cloud services to sustain innovation.
\end{itemize}

Operational excellence in IT contexts extends beyond efficiency to encompass agility, compliance, and culture. The categories identified provide a structured representation of OpEx in digital settings and serve as a conceptual bridge from the literature review to the requirements engineering in Section~\ref{subsec:re-sa}. By grounding requirements in these categories, the thesis ensures that the subsequent multi-agent architecture design directly reflects proven enablers of operational excellence in IT firms.

\subsection{Workflow Automation}\label{subsec:workflow-auto}
Building on the need to balance stability and agility in operational processes, workflow automation emerged as a means to systematically coordinate and streamline business workflows. It refers to the use of software systems (workflow management systems, WfMS) to orchestrate tasks, information flows, and decisions along a predefined business process model. According to industry standards, workflow automation entails routing documents, information, or tasks between participants according to procedural rules, with the goal of reducing manual effort and variability in execution \parencite{basuResearch2002}. Early WfMS in the late 20th century were designed to make work more efficient, to integrate heterogeneous applications, and to support end-to-end processes even across organizational boundaries \parencite{stohrWorkflow2001}.

By encoding business procedures into formal process models that are executed by a central \emph{workflow engine}, organizations could enforce consistent process flows, improve speed and accuracy, and embed compliance checks into routine operations. In essence, pre-AI workflow automation provided a structured, deterministic way to implement business processes in software, directly addressing chronic issues like fragmented manual tasks and data silos in pursuit of operational excellence \parencite{basuResearch2002}.

One foundational aspect of workflow automation is \textsc{process orchestration}. Orchestration denotes the centralized coordination of tasks and activities according to a defined process logic. In a typical WfMS, a workflow engine enacts the process model, dispatching tasks to the right resources (human or machine) in the correct sequence and enforcing the business rules at each step \parencite{basuResearch2002}. This engine-driven coordination brings predictability and repeatability to workflows: tasks are executed in a fixed, optimized order with minimal ad-hoc variation. By systematically controlling task flow, early workflow systems could eliminate many manual hand-offs and delays, thereby boosting efficiency and consistency in outcomes \parencite{stohrWorkflow2001}. The orchestration approach essentially translated managerial routines into software: for example, an order processing workflow would automatically route an order through credit check, inventory allocation, shipping, and billing steps without needing human coordination at each transition. Such deterministic sequencing was crucial for achieving the quality and reliability targets of operational excellence in an era before adaptive AI capabilities.

A closely related design concern is \textsc{integration}. Workflow automation inherently requires linking together diverse people, departments, and IT systems into an end-to-end process. Literature emphasizes that WfMS must integrate heterogeneous application systems and data sources to allow seamless information flow across functions \parencite{stohrWorkflow2001}. For instance, a procurement workflow might connect an ERP inventory module, a supplier's database, and a financial system so that each step can automatically consume and produce the necessary data. This integration extends beyond technical connectivity; it also encompasses coordinating work across organizational boundaries. As e-business initiatives grew in the 1990s and early 2000s, workflows increasingly spanned multiple organizations (suppliers, partners, customers), demanding inter-organizational process integration \parencite{basuResearch2002}. Research in this period identified the need for distributed workflow architectures that could bridge independent systems and companies.~\textcite{georgakopoulosOverview1995} noted that existing workflow tools had limitations in complex environments, calling for infrastructure to handle “heterogeneous, autonomous, and distributed information systems”. In practice, this led to the development of interoperability standards (e.g., XML-based process definitions, web service interfaces) and process choreography protocols to ensure that a workflow could progress smoothly even when multiple organizations or platforms were involved. Effective integration was thus an essential condition for workflow automation, enabling the end-to-end automation of processes that formerly stopped at organizational or system boundaries.

To manage complexity and change, \textsc{modularity} in workflow design became another important principle. Rather than hard-coding monolithic process flows, architects sought to break workflows into modular components or sub-processes that could be reused and reconfigured as needed. This component-based approach was accelerated by the rise of service-oriented architectures and e-business “workflow of services” concepts. For example, composite e-services and e-hubs allow organizations to construct complex workflows by composing smaller service modules. A modular workflow architecture improves maintainability: if a business rule changes or a new subprocess is required, one can update or insert a module without redesigning the entire workflow from scratch. Modularity also underpins adaptability. Ideally, a workflow systematizes routine functions but can be adjusted to accommodate new requirements or variations in the process \parencite{basuResearch2002}. In other words, the literature suggests that well-designed workflow automation should combine standardization with flexibility: processes are structured into clear modules for the “happy path” of routine operations, yet those modules can be reorchestrated or overridden in exceptional cases. This design philosophy reflects an early recognition that no single process model can anticipate all future conditions, so a degree of configurability must be built in.

Despite efforts to introduce flexibility, traditional workflow automation faced notable challenges with \textsc{exception handling}. Exception handling refers to the ability of a system to cope with deviations, errors, or unforeseen scenarios that fall outside the predefined process flow.~\textcite{basuResearch2002} candidly observe that existing WfMS “tend to fall short whenever workflows have to accommodate exceptions to normal conditions”---i.e., when something unexpected occurs that was not explicitly modeled, the system often cannot resolve it autonomously, forcing human intervention. Typically, designers might anticipate a limited number of exception scenarios and build alternate paths for those (e.g., an approval escalation if a manager is absent). However, if a novel exception arises (say, a new regulatory requirement or an unplanned system outage affecting a step), the rigid workflow cannot handle it, and manual workarounds are needed. This brittleness of early workflows under dynamic conditions was widely acknowledged. Research proposed various approaches to improve exception handling, such as more advanced process metamodels and integration of AI or rule-based decision support to catch and respond to anomalies \parencite{basuResearch2002}. Yet, in the pre-AI era, most workflow automation remained predominantly rule-driven and inflexible outside of predefined contingencies. Exception handling thus stood out as a critical limitation of classical automation approaches, highlighting a gap between the desire for end-to-end automation and the reality of complex, ever-changing business environments.

Another salient theme in the literature is \textsc{workflow governance}---the structures and mechanisms for overseeing automated workflows and aligning them with organizational policies. As companies entrusted core business processes to software, ensuring the correct and intended execution of those processes became vital. Key governance considerations include monitoring, auditing, and controlling workflows. A WfMS typically provides monitoring dashboards and logs so that managers can track the state and performance of process instances (e.g., to identify bottlenecks or errors). It also enforces role-based access control, ensuring that only authorized personnel perform certain tasks or approvals, which is essential for compliance in regulated industries.~\textcite{basuResearch2002} highlight the importance of organizational “metamodels” and control mechanisms that tie workflows to an enterprise’s structure – for example, defining which organizational roles are responsible for each task and how escalation or overrides should happen under specific conditions. Additionally, governance extends to establishing standards and best practices for workflow design and deployment. Industry coalitions and standards bodies (such as the WFMC in the 1990s) issued reference models and interface standards to promote consistency and interoperability in workflow implementations. In the context of inter-organizational workflows, governance also means agreeing on protocols and service-level commitments between partners so that automated interactions remain trustworthy and transparent. Overall, robust governance in workflow automation ensures not only efficiency but also accountability, security, and compliance. It addresses the managerial and oversight challenges that arise once processes are no longer directly handled by individuals but by software agents following prescribed logic.

From the structured analysis of these sources, pre-AI workflow automation can be summarized by a set of key architectural categories. These represent the dominant design objectives and constraints that had to be addressed in classical workflow systems, and they mirror the strengths as well as the limitations of those systems. The QCA-driven review for this thesis distilled the following categories (both deductively and inductively derived) as particularly relevant:

\begin{itemize}
  \item \textsc{process orchestration:} Centralized control of process execution through a workflow engine, which dispatches tasks and enforces business rules to ensure activities occur in the correct sequence.
  \item \textsc{integration and interoperability:} Seamless linking of diverse applications, data sources, and organizational units into a unified process flow, often via standardized interfaces or protocols, so that automation spans across technological and organizational boundaries.
  \item \textsc{modularity and reusability:} Composition of workflows from modular tasks or sub-processes that can be reused and reconfigured, allowing the process design to be adapted or extended with minimal effort.
  \item \textsc{exception handling and flexibility:} Mechanisms to detect and manage deviations or unexpected situations in a workflow, enabling the system to handle errors or novel scenarios (or escalate them appropriately) rather than simply failing.
  \item \textsc{workflow governance:} Oversight and management of workflows through monitoring, audit trails, and role-based controls, ensuring that automated processes remain aligned with business policies, performance targets, and regulatory requirements.
\end{itemize}

The pre-AI workflow automation literature established a foundation of structured, rule-driven process management focused on efficiency, integration, and control. The categories above encapsulate both the core capabilities that made traditional workflow systems valuable and the pain points (like inflexibility in the face of change) that constrained their applicability. These insights provide a structured basis for the requirements derivation in Section~\ref{sec:mod-req} and guide the architectural considerations in later chapters. By grounding the design in these well-understood aspects of workflow automation, the thesis ensures that the proposed multi-agent architecture builds on proven practices while also targeting the gaps. Indeed, many of the limitations noted here---especially around exception handling and adaptiveness---motivate the incorporation of agentic AI elements in the next section (3.3), which explores how intelligent agents can augment and transform workflow automation to better achieve operational excellence.

\subsection{Agentic Artificial Intelligence}\label{subsec:agentic-ai} 
Agentic AI refers to systems composed of multiple interacting generative agents that autonomously collaborate to achieve complex goals. It represents a shift beyond single “AI agents” toward orchestrated multi-agent ecosystems, enabled largely by recent generative AI advances. Whereas traditional automation (e.g.~rule-based RPA) executes predefined steps, an agentic architecture features \emph{adaptive, goal-directed agents} that can perceive context, make decisions, and act with minimal hard-coded instructions. This idea builds on classic MAS principles of autonomy and social action \parencite{castelfranchiModelling1998,ferberMultiagent1999}, but now agents are augmented with learning and reasoning capabilities from large language models (LLMs).~\textcite{sapkotaAI2026} frame agentic AI as \textit{“multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy”} in pursuit of flexible problem-solving.

A defining trait of agentic AI is a high degree of \textsc{autonomy}. Agents can operate without constant human or central control, making and executing decisions in real time. Early MAS research already emphasized agent autonomy---e.g.~agents as entities with independent control over their actions and state. Modern generative agents greatly amplify this autonomy by leveraging LLM-based reasoning to plan multi-step actions toward goals. For instance, frameworks like AutoGPT demonstrated that a single LLM-based agent can iteratively break down objectives, choose actions, and adjust based on feedback without human intervention \parencite{sapkotaAI2026}. This autonomy promises agility and decision quality (agents can respond to situational changes or large search spaces beyond rigid scripts), but it also introduces risks. As \textcite{russellResearch2015} caution, each additional decision delegated to an opaque AI agent shifts “ethical control” away from human operators. In enterprise settings, uncontrolled autonomous decisions might lead to policy violations or unsafe actions \parencite{gauravGovernance2025}. Thus, an architectural challenge is balancing agent freedom with mechanisms to supervise or constrain critical decisions. In practice, this means designing agents with clearly scoped authorities, fail-safes, or escalation paths (e.g.~requiring human confirmation for high-impact actions) to align autonomy with organizational policies.

\textsc{tool-use capability} is another hallmark of agentic AI architectures. Simply put, agents are not limited to their built-in knowledge; they can invoke external tools, APIs, or other services as part of their reasoning loop. This extends an agent's functionality---for example, an AI agent might call a database, run a code snippet, or query web services to gather real-time information. Research shows that augmenting LLM agents with tool integration significantly improves their problem-solving scope and accuracy. Notably, the \emph{ReAct} paradigm interleaves an agents chain-of-thought with tool calls, allowing it to \emph{perceive} (via queries) and \emph{act} (via external operations) iteratively. Such designs transform static LLMs into \emph{dynamic cognitive agents} that can “perceive, plan, and adapt”, a critical capability for complex, multi-step workflows. For workflow automation, this means an agent can not only parse instructions but also execute parts of a workflow (e.g.~trigger an RPA bot or send an alert) and then reason over the results. Architecturally, enabling tool use requires adding interface layers for the agent to safely interact with enterprise systems (APIs, databases, RPA scripts), along with policies on allowed tools. It's worth noting that tool integration adds orchestration complexity and potential error propagation paths \parencite{sapkotaAI2026}. Therefore, designs often include an \emph{orchestration layer} or planner agent that manages when and how tools are invoked, checks tool outputs, and handles exceptions (e.g.~what if a tool fails or returnsunexpected data). In summary, tool-use greatly enhances agent capabilities, but it demands careful architectural planning to manage the added complexity and ensure robust tool-agent interaction. 

Agentic AI systems are inherently \emph{multi-agent}---they comprise not one but many agents, often with specialized roles, that must \textsc{coordinate} their efforts. This multi-agent approach stems from the insight that complex workflows can be decomposed: instead of one monolithic AI trying to do everything, a team of agents can each handle subtasks and then combine results. Such \textsc{specialization} aligns with principles of operational excellence (e.g.~division of labor and expertise) and has been shown to improve performance. For example, \textcite{shuEffective2024} report that a collaborative team of LLM-based agents achieved up to 70\% higher success rates on complex tasks compared to a single-agent approach. Architecturally, coordination mechanisms are crucial to harness these gains. Agents need to communicate their intentions, share data/results, and synchronize plans.

The literature distinguishes \emph{coordination structures} along two dimensions: \emph{hierarchy} vs.~\emph{flat} and \emph{centralized} vs~\emph{decentralized} decision making. In a centralized hierarchical design, a top-level \emph{planner/manager} agent delegates tasks to subordinate agents and integrates their outputs (akin to a project manager overseeing specialists). This can simplify global coordination and ensure alignment with a single source of truth (the planner's goal), at the cost of a single point of failure or bottleneck. Conversely, decentralized teams use peer-to-peer negotiation or voting; all agents are more equal and collectively decide on task assignments or conflict resolution (drawing on concepts from distributed AI and game theory). For instance, one recent system had developer agents jointly agree on a solution design without a central boss, mimicking consensus decision-making \parencite{xuTransparency2023}. Each approach has trade-offs: hierarchical control can be more efficient for well-structured processes, while decentralized collaboration may be more robust to single-agent failure and better for ill-structured problems. Inter-agent communication protocols (what messages agents send and when) are another design facet---simple cases use direct message passing or shared memory, whereas more complex setups might use an event-bus or blackboard architecture for asynchronous communication.

Importantly, specialization means each agent can be bounded in scope (e.g.~a “Compliance Checker” agent vs.~a “Data Retrieval” agent), which helps with scalability: each agent's LLM or reasoning module can operate within a focused context window, and different team members can even use different model types suited to their niche. This modularity and specialization, orchestrated through well-defined coordination logic, is a key architectural strength of agentic AI.~It mirrors how human organizations structure teams for efficiency, and indeed is crucial for aligning multi-agent AI workflows with complex enterprise processes. 

As agent behaviors become more autonomous and distributed, ensuring \textsc{observability} of the system is vital. Observability here means that the internal states, decisions, and actions of agents can be monitored and understood by humans or supervisory systems. Traditional MAS literature often dealt with observability in terms of state visibility (e.g.~in partially observable environments), but in an enterprise context it translates to runtime transparency and traceability of what agents are doing and why. One challenge is that LLM-driven agents reason in natural language (or latent vectors), making their decision process somewhat opaque. Recent studies highlight that AI agents “lack transparency, complicating debugging and trust”, and they advocate for robust logging and auditing pipelines to make agent operations inspectable. In practice, agentic architectures include components to log key events: each prompt an agent generates, each tool API call and its result, each decision or plan the agent commits to, etc. Such audit logs enable post-hoc analysis, error tracing, and explanations---for example, if a workflow failed or a compliance issue occurred, developers can replay the agent interactions to pinpoint the cause. 

Some frameworks even expose an agent's chain-of-thought (the intermediate reasoning steps) in a controlled way for debugging or compliance review. Beyond logging, observability can be enhanced through dashboarding and alerts: e.g.~real-time monitors that track agent performance metrics or detect anomalies (like an agent taking too long on a task or generating an out-of-bounds output). The end 13 14 15 16 16 17 17 2 goal is to treat an agentic AI system not as a “black box” automation, but as an observable workflow that operations teams can supervise akin to any critical IT system. This also ties into explainability: by capturing the rationale behind decisions (even if only in approximate form, such as storing the intermediate reasoning text), the system can later provide explanations for its actions, which is invaluable for trust and for continuous improvement. Overall, the literature suggests that designing for transparency---“instrumenting” agents with logging, and perhaps even designing agents to self-report
their status---is a best practice to ensure agentic AI doesn't become an inscrutable tangle of
automations. High observability supports OpEx principles by enabling traceability, accountability, and faster incident response when something goes wrong.

Finally, a recurrent theme is the need for strong \textsc{governance} mechanisms in agentic AI architectures to ensure alignment with rules, ethics, and organizational policies. By their nature, autonomous agents may produce unexpected or undesired outcomes---a risk amplified in multi-agent settings where interactions are complex and no single agent has full oversight. Without proper governance, an agentic system could easily violate compliance requirements or strategic constraints, undermining operational excellence goals (e.g.~a well-intentioned agent might inadvertently expose sensitive data or execute an unauthorized transaction). In fact, \textcite{gauravGovernance2025} warn that the “absence of scalable, decoupled governance remains a structural liability” in today's agentic AI ecosystems. To address this, researchers are exploring policy-enforcement layers that sit between the agents and the outside world. One such approach is Governance-as-a-Service (GaaS), a framework that intercepts agent actions at runtime and checks them against explicit rules or constraints. Rather than trusting each agent to self-regulate, an external governance layer can block or redirect high-risk actions, log rule violations, and even adapt penalties or restrictions on agents that exhibit misbehavior over time. This effectively creates an oversight controller for the multi-agent system---analogous to a “compliance officer” in a human organization---that ensures no single agent can compromise the system's integrity. Key design elements include declarative policy rules (defining allowable vs.~disallowed outputs or tool uses), a mechanism to monitor all agent outputs (to flag violations), and possibly a trust score or reputation model to quantify an agent's reliability based on past behavior. Beyond automated enforcement, governance also encompasses human oversight: for example, requiring human approval for certain agent decisions (human-in-the-loop checkpoints) or having a fallback where a human operator can intervene if the agents encounter an ambiguous ethical situation. In classical MAS research, analogous concepts existed like normative agents and electronic institutions that enforce “rules of engagement” among agents; the new twist is that with LLM-based agents we must often treat the models as black boxes, so governance can't be injected into their internal logic easily and must surround them instead. The overarching recommendation is that any architecture for generative multi-agent workflows should bake in governance from the start---not as an afterthought---to manage risk. As one author succinctly put it, such a system “does not teach agents ethics; it enforces them”. This governance emphasis aligns tightly with Operational Excellence goals of compliance, risk management, and trustworthiness. 

In summary, the literature portrays agentic AI as a powerful paradigm for workflow automation that, if well-designed, can dramatically enhance agility, adaptability, and decision quality in operations. By combining autonomous, tool-using agents into coordinated architectures, organizations can automate complex processes that previously required human judgment. At the same time, achieving sustainable excellence with such systems demands careful attention to transparency and control: architects must ensure agents remain observable and governable to uphold compliance and reliability standards. These insights set the stage for the next section of this thesis, which will integrate Operational Excellence principles, workflow automation requirements, and agentic AI capabilities into a unified reference architecture. The themes of autonomy, coordination, and governance identified here directly inform the design choices and requirements elaborated in the subsequent chapters.

Using Mayring's QCA approach, five key themes emerge regarding the architectural implications of agentic AI:
\begin{itemize}
    \item \textsc{autonomy in decision-making:} Agents operate with high independence, making real-time decisions without constant human oversight, which enhances agility but requires careful scope definition to manage risks.
    \item \textsc{tool use and integration:} Agents can invoke external tools, APIs, or services to extend their capabilities, necessitating robust interface layers and error handling mechanisms to ensure reliable interactions.
    \item \textsc{coordination \& specialization:} Multi-agent systems benefit from structured coordination mechanisms (hierarchical or decentralized) and role specialization to efficiently tackle complex workflows through division of labor.
    \item \textsc{observability \& transparency:} Ensuring that agent actions and decisions are logged, monitored, and explainable is critical for trust, debugging, and compliance, requiring comprehensive auditing and real-time monitoring tools.
    \item \textsc{governance \& compliance:} Strong oversight mechanisms, including policy enforcement layers and human-in-the-loop checkpoints, are essential to align agent behaviors with organizational rules and ethical standards, mitigating risks associated with autonomous decision-making.
\end{itemize}

\section{Modeling the Requirements}\label{sec:mod-req}
\subsection{[placeholder]Functional Requirements}\label{subse:func-req}
\subsection{[placeholder]Quality Requirements}\label{subse:qua-req}
\subsection{[placeholder]Constraints}\label{subse:const}

\section{Modeling the Architecture}\label{sec:mod-mas}
\begin{listing}[h]
    \caption{Excerpt of the Requirements model}
    \inputminted[firstline=1,lastline=25]{text}{ressources/models/requirements.sysml}
\end{listing}
\subsection{[placeholder]Modeling the Agents}\label{subse:mod-agents}
\subsection{[placeholder]Modeling the Architecture}\label{subse:mod-arch}
\subsection{[placeholder]Modeling the Interactions}\label{subse:mod-interactions}

\section{Discussion: Applicability Criteria}\label{sec:app-criteria}
    
\section{Conclusion}
Future work should extend this conceptual design into practical evaluation and implementation. In particular, empirical validation of the architecture in industry settings, tool-supported instantiation in SysML, and comparative studies against traditional workflow automation would provide valuable evidence of its applicability and impact. Further, integrating additional agentic AI capabilities such as autonomous negotiation or explainability could enhance both usability and compliance assurance.
\clearpage