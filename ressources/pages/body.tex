\section{Introduction}\label{sec:intro} % finished
Organizations across industries continue to face persistent challenges in achieving operational excellence (OpEx). Fragmented processes, manual interventions, and inconsistent data quality undermine efficiency and decision-making. Legacy workflows and siloed systems exacerbate these inefficiencies, while traditional automation approaches often lack the adaptability needed in dynamic business environments. For companies, this translates into slower response times, higher compliance risks, and limited scalability—issues that directly threaten competitiveness.

Agentic AI, building on the advances of generative artificial intelligence (GenAI), opens new possibilities to extend automation beyond deterministic scripts. While GenAI provides the cognitive and generative capabilities, agentic AI leverages these to create adaptive, tool-using agents that can plan, act, and coordinate---thereby supporting governance, decision quality, and organizational agility. Despite this potential, both practice and academic literature lack structured strategies and conceptual frameworks for embedding such agentic capabilities into operational workflows in a scalable and value-driven way. This gap motivates the present research.

In this context, multi-agent systems (MAS) can serve as a reference architecture for integrating GenAI-enabled agentic AI into enterprise workflow automation. The central research question is:

\vspace{0.5\baselineskip}
\emph{How can a MAS architecture be designed to integrate GenAI capabilities into workflow automation, in order to enhance agility, compliance, and decision quality to achieve OpEx?}
\vspace{0.5\baselineskip}

To answer this question, the study addresses the following sub-questions:
\begin{itemize}
    \item \emph{Which design requirements are necessary to align a multi-agent architecture with the goals of OpEx?}
    \item \emph{How should a MAS be architected to fulfill these requirements?}
    \item \emph{Under which conditions is deploying a generative multi-agent architecture justified over traditional automation approaches?}
\end{itemize}

Methodologically, the thesis applies Design Science Research (DSR) to develop a conceptual reference architecture. The approach synthesizes requirements from academic literature and OpEx principles, models agent roles and interactions, and derives applicability conditions for real-world deployment.

The core contribution of this work is a conceptual design of a MAS that leverages GenAI to support OpEx in enterprise workflows. Specifically, it delivers:
\begin{itemize}
    \item \emph{A structured synthesis of system requirements derived from academic literature and OpEx principles.}
    \item \emph{A conceptual architecture detailing agent roles, interactions, and integration points.}
    \item \emph{A set of applicability conditions and design considerations to guide future deployment and evaluation of generative multi-agent architectures in practice.}
\end{itemize}

The scope is limited to conceptual design; formal evaluation and technical implementation are proposed as future work. Although the architecture is designed to remain industry-agnostic, a use case from the financial services sector is introduced to illustrate how the conceptual model can be instantiated in a regulated, legacy-intensive environment.

After this introduction in Section~\ref{sec:intro}, the thesis is structured as follows: Section~\ref{sec:method} outlines the research methodology, including the use of Design Science Research (DSR) and supporting methods. Section~\ref{sec:lit-rev} presents a literature review on operational excellence, workflow automation, and agentic AI.~Section~\ref{sec:mod-req} details the synthesis and modeling of requirements. Section~\ref{sec:mod-mas} develops the conceptual multi-agent architecture. Section~\ref{sec:discussion} discusses the applicability of MAS in workflow automation use cases, and Section~\ref{sec:conclussion} concludes with reflections and directions for future research.

\section{Methodology}\label{sec:method}
This thesis applies DSR methodology to create a conceptual artifact---a multi-agent architecture for workflow automation. Practically, the approach unfolded in three steps: (1) \emph{reviewing the literature} on OpEx, workflow automation, and agentic AI;~(2) \emph{deriving and structuring requirements} from literature and case material into a requirements model; and (3) \emph{designing a conceptual system architecture} using Systems Modeling Language (SysML).

Supporting methods included Mayring-style qualitative content analysis (QCA) for the review, requirements engineering (RE) and systems analysis for the requirements model, and information systems design (ISD) to structure the architecture and ensure requirement-to-design traceability, supported by SysML modeling practices from Model-Based Systems Engineering (MBSE). Within DSR, the work focuses on problem identification, objective definition, and conceptual design, while instantiation/demonstration and formal evaluation are out of scope given the bachelor-thesis format and resource constraints. This scoping maintains methodological rigor while keeping the contribution focused: a well-argued reference architecture ready for subsequent implementation and empirical evaluation.

\subsection{Qualitative Content Analysis}\label{subsec:qca} % finished
To ensure a systematic and structured literature review, this thesis employed QCA following the principles of \textcite{mayringQualitative2022}. As a rule-based method for synthesizing insights from textual sources, QCA was used within the DSR framework to support the problem identification and objective definition phases \parencite{hevnerDesign2004,peffersDesign2007}. In this thesis, it was applied in a literature-focused manner to structure the review and provide a traceable basis for subsequent RE.\

The analysis was scoped along three dimensions. The \emph{analysis unit} was defined as the overall body of literature addressing operational excellence, workflow automation, and agentic AI.\ The \emph{context unit} consisted of individual publications (books, peer-reviewed articles, industry reports, standards). The \emph{coding unit} was defined as discrete statements or conceptual claims relevant to the intersection of OpEx, automation paradigms, and AI-based multi-agent systems. 

A mixed deductive-inductive approach was used. Deductive categories were derived from established theory, including OpEx dimensions such as adaptability, compliance, and decision quality, as well as prior automation frameworks (e.g., Robotic Process Automation, RPA;\ Intelligent Process Automation, IPA). Inductive categories emerged from the material itself, capturing issues highlighted repeatedly in the sources, such as observability, traceability, and governance in agentic AI.\ This balance ensured that both established and novel concerns were systematically reflected.

The outcome of this categorization was not a formal codebook, but a set of thematic clusters that guided the narrative structure of Section~\ref{sec:lit-rev}. Each subsection of the literature review is organized around these categories, which in turn serve as the input for the elicitation lists presented at the beginning of Section~\ref{sec:mod-req}. In this way, QCA provides both a conceptual ordering of the literature and a direct bridge into the requirements engineering process.

\subsection{Requirements Engineering}\label{subsec:re-sa} % finished
Following the \textcite{IEEEStandard1990} definition, a requirement is a \emph{condition or capability needed by a user to solve a problem or achieve an objective}. RE provides the systematic means to derive such objectives. In this thesis, RE was applied in the early phases to ensure that the conceptual architecture rests on precise, validated needs rather than general aspirations.

The synthesis of requirements followed a structured but literature-driven process. Recurring design concerns were identified from the results of the QCA, documented in \emph{elicitation lists}, and then consolidated into a unified set of requirement candidates. Consistent with \textcite{glinzHandbook2020}, each candidate was reformulated into an atomic, unambiguous, and verifiable “shall” statement and classified into \emph{functional requirements} (system behaviors), \emph{quality requirements} (non-functional attributes such as performance or compliance), or \emph{constraints} (technological or regulatory limits).

While this procedure draws on the phases described by \textcite{herrmannGrundlagen2022} (elicitation, documentation, analysis, management), it was adapted to the scope of this thesis: instead of stakeholder workshops, the primary elicitation source was the systematically coded literature. To situate the requirements, a complementary systems analysis defined the system boundary, identified stakeholders and external actors, and clarified interface obligations---helping prevent scope creep and omissions.

Requirements were then represented in SysML requirement diagrams.~Trace links connect each documented requirement to the respective architecture elements, enabling full requirement-to-design traceability via \texttt{«satisfy»} and \texttt{«verify»} relationships. This model-based approach ensures that design decisions can always be traced back to validated needs and that no requirement was overlooked.

In summary, the integration of RE and systems analysis provided a structured, traceable, and quality-assured requirement set. This foundation anchors the subsequent conceptual architecture in rigorously defined objectives, ensuring consistency with both DSR methodology and operational excellence goals.

\subsection{Information Systems Design}\label{subsec:isd}
In line with the DSR approach, this thesis models the architecture in SysML~v2 and applies MBSE practices to ensure requirements-to-design traceability. Although MBSE originates in systems engineering, its discipline transfers well to information systems and supports the systematic development of conceptual architectures. As MAS grow in scope and complexity, a formal modeling framework becomes essential for maintaining control, observability, and consistency. SysML~v2 is explored here as that framework, providing precise semantics and a version-controllable, analyzable representation of the artifact.

MBSE is used to transform the synthesized requirements into a coherent, analyzable system model and to validate the design at the conceptual level. Concretely, each requirement is represented as a SysML~\texttt{«requirement»} element and linked via \texttt{«satisfy»} and, where applicable, \texttt{«verify»} relations to architectural elements (agent roles, interactions, and policies). This establishes requirements-to-design traceability and enables early checks against stakeholder needs and constraints. The resulting model offers a unified view of structure and behavior, making interface obligations, coordination patterns, and exception paths inspectable before implementation.

The conceptual architecture is captured as a SysML~v2 model using a plain-text, Eclipse-based workflow. SysML~v2's textual notation enables precise, tool-agnostic definitions of structure and behavior under version control. Compared with SysML~v1 and informal diagramming, SysML~v2 offers clearer semantics and richer constructs for specifying MAS concerns such as coordination patterns, policy enforcement points, interfaces, and exception pathways. In this work, the SysML~v2 model functions as a machine-interpretable blueprint of the architecture: it supports early validation against the requirements, consistent terminology and interfaces, and a stable foundation for subsequent instantiation and evaluation.

\section{Literature Review}\label{sec:lit-rev}
The literature review is organized into three subsections that together frame the problem context of this thesis. It begins with operational excellence, which defines the strategic objectives—adaptability, compliance, decision quality—that guide enterprise transformation efforts. The second subsection addresses workflow automation, as the established technological approach for operationalizing these objectives in practice. The third subsection turns to agentic AI, a rapidly emerging paradigm that extends automation beyond deterministic scripts toward adaptive, tool-using agents. This sequence—objectives, established solutions, emerging solutions—provides a logical progression from strategic goals to current practice and then to prospective innovations. It ensures that the requirements synthesized in Section~\ref{sec:mod-req} are grounded in both enduring management principles and the latest technological developments relevant to workflow design.

\subsection{Operational Excellence}\label{subsec:op-ex}
OpEx originated as a management philosophy in the manufacturing sector, particularly in the automotive industry to optimize quality and efficiency. In this classical context, OpEx focused on minimizing defects, eliminating waste, and embedding continuous improvement practices into organizational routines \parencite{womackLean1997}. While these roots remain important, they provide only a partial foundation for understanding OpEx in today's IT-driven enterprises, which operate in volatile environments shaped by rapid technological change, regulatory complexity, and global competition.

One central dimension of OpEx is \textsc{adaptability and agility}. In IT-driven firms, where OpEx is defined less by physical production flows and more by the ability to execute strategies effectively while maintaining innovation, adaptability refers to the capacity of processes to be reconfigured in response to volatility, ensuring resilience in dynamic environments. Agility emphasizes change-readiness and rapid adaptation, qualities increasingly recognized as indicators of organizational excellence in globalized and unpredictable markets. While adaptability enhances resilience, it can reduce efficiency if frequent changes disrupt standardization; conversely, a strong focus on efficiency can make processes rigid and less responsive to unexpected events. In practice, organizations must reconcile continuous improvement with agility, balancing stability and flexibility in their operational routines \parencite[cf.][p. 1599]{carvalhoOperational2023}.

Another recurring theme in the literature is \textsc{compliance and risk management}. OpEx in regulated industries demands that workflows embed mechanisms for ensuring transparency, auditability, and regulatory adherence. Compliance safeguards minimize operational risk but can introduce bureaucratic overhead and slow decision-making. The key challenge is balancing strict rule enforcement with the flexibility needed to respond to novel business conditions, a tension especially visible in digital service environments with evolving legal frameworks \parencite[cf.][p. 687]{owoadeSystematic2024}.

A further category is \textsc{decision quality}. A core aim of OpEx is not only to accelerate decision-making but to improve its reliability and evidential grounding \parencite[cf.][p. 685]{owoadeSystematic2024}. Automation can help by aggregating relevant data and reducing errors, but it also risks opacity and overconfidence when human oversight is limited. The central trade-off is speed versus quality: excessive automation may produce faster but less accountable outcomes, while excessive oversight slows operations. For sustainable excellence, systems must therefore support evidence-based choices and make decision pathways transparent \parencite{carvalhoOperational2023}.

Another key dimension is \textsc{efficiency and continuous improvement}. Rooted in Lean and TQM traditions, efficiency emphasizes minimizing waste and reducing manual effort, while continuous improvement institutionalizes iterative refinements in processes. Together, these principles enhance operational reliability and cost-effectiveness, yet they can conflict with the need for flexibility in volatile environments. The challenge is to design workflows that are optimized for today while remaining adaptable for tomorrow \parencite[cf.][p. 14.16 \& p. 8.1]{juranQuality1999}.

Customer-facing outcomes are captured by \textsc{customer-centricity}. OpEx emphasizes that processes must be aligned with user needs and service-level commitments, ensuring reliability, responsiveness, and satisfaction. A strong customer focus can create pressure to customize and accelerate processes, which may undermine efficiency or compliance. The literature stresses that sustainable excellence requires balancing external demands with internal consistency \parencite{womackLean1997,juranQuality1999}.

The category of \textsc{user empowerment and culture} recognizes that operational excellence is not solely technical but also organizational. Effective improvement requires systems that support transparency, collaboration, and employee engagement. Empowerment fosters ownership and participation, but decentralizing authority can introduce inconsistency and conflict with standardization goals. Culture therefore functions as both an enabler and a constraint for process excellence, shaping how automation is accepted and leveraged by human actors \parencite[cf.][p. 15.2-3]{juranQuality1999} \parencite[cf.][p.3]{womackLean1997}.

Finally, \textsc{technology integration and scalability} reflects the increasing role of digital platforms in enabling OpEx. Modern enterprises rely on architectures that integrate automation, AI, and cloud services to achieve scalable and resilient operations \parencite{owoadeSystematic2024}. Integration enables end-to-end process coverage and agility, but also increases dependency on heterogeneous systems and external vendors. Scalability promises growth and innovation, yet without careful governance it can amplify complexity and risk.

\subsection{Workflow Automation}\label{subsec:workflow-auto}
Building on the need to balance stability and agility in operational processes, workflow automation emerged as a means to systematically coordinate and streamline business workflows. It refers to the use of software systems (workflow management systems, WfMS) to orchestrate tasks, information flows, and decisions along a predefined business process model. According to industry standards, workflow automation entails routing documents, information, or tasks between participants according to procedural rules, with the goal of reducing manual effort and variability in execution \parencite[cf.][p. 2]{basuResearch2002}. Early WfMS in the late 20th century were designed to make work more efficient, to integrate heterogeneous applications, and to support end-to-end processes even across organizational boundaries \parencite[cf.][p. 281]{stohrWorkflow2001}.

One foundational aspect of workflow automation is \textsc{process orchestration}. By encoding business procedures into formal process models that are executed by a \emph{workflow engine}, organizations could enforce consistent process flows, improve speed and accuracy, and embed compliance checks into routine operations. In essence, pre-AI workflow automation provided a structured, deterministic way to implement business processes in software, directly addressing chronic issues like fragmented manual tasks and data silos in pursuit of OpEx \parencite[cf.][p. 231]{gadatschGrundkurs2012}. Orchestration denotes the centralized coordination of tasks and activities according to a defined process logic. In a typical WfMS, a workflow engine enacts the process model, dispatching tasks to the right resources (human or machine) in the correct sequence and enforcing the business rules at each step \parencite{basuResearch2002}. This engine-driven coordination brings predictability and repeatability to workflows: tasks are executed in a fixed, optimized order with minimal ad-hoc variation. By systematically controlling task flow, early workflow systems could eliminate many manual hand-offs and delays, thereby boosting efficiency and consistency in outcomes. The orchestration approach essentially translated managerial routines into software: for example, an order processing workflow would automatically route an order through credit check, inventory allocation, shipping, and billing steps without needing human coordination at each transition. Such deterministic sequencing was crucial for achieving the quality and reliability targets of OpEx in an era before adaptive AI capabilities \parencite[cf.][pp. 283-285]{stohrWorkflow2001}.

A closely related design concern is \textsc{integration}. Workflow automation inherently requires linking together diverse people, departments, and IT systems into an end-to-end process. Literature emphasizes that WfMS must integrate heterogeneous application systems and data sources to allow seamless information flow across functions \parencite[cf.][pp. 289-290]{stohrWorkflow2001}. For instance, a procurement workflow might connect an ERP inventory module, a supplier's database, and a financial system so that each step can automatically consume and produce the necessary data. This integration extends beyond technical connectivity; it also encompasses coordinating work across organizational boundaries. As e-business initiatives grew in the 1990s and early 2000s, workflows increasingly spanned multiple organizations (suppliers, partners, customers), demanding inter-organizational process integration \parencite{basuResearch2002}. Research in this period identified the need for distributed workflow architectures that could bridge independent systems and companies.~\textcite{georgakopoulosOverview1995} noted that existing workflow tools had limitations in complex environments, calling for infrastructure to handle heterogeneous, autonomous, and distributed information systems. In practice, this led to the development of interoperability standards (e.g., XML-based process definitions, web service interfaces) and process choreography protocols to ensure that a workflow could progress smoothly even when multiple organizations or platforms were involved. Effective integration was thus an essential condition for workflow automation, enabling the end-to-end automation of processes that formerly stopped at organizational or system boundaries.

To manage complexity and change, \textsc{modularity} in workflow design became another important principle. Rather than hard-coding monolithic process flows, architects sought to break workflows into modular components or sub-processes that could be reused and reconfigured as needed. This component-based approach was accelerated by the rise of service-oriented architectures and e-business \enquote{workflow of services} concepts. For example, composite e-services and e-hubs allow organizations to construct complex workflows by composing smaller service modules. A modular workflow architecture improves maintainability: if a business rule changes or a new subprocess is required, one can update or insert a module without redesigning the entire workflow from scratch. Modularity also underpins adaptability. Ideally, a workflow systematizes routine functions but can be adjusted to accommodate new requirements or variations in the process \parencite[cf.][p. 10]{basuResearch2002}. In other words, the literature suggests that well-designed workflow automation should combine standardization with flexibility: processes are structured into clear modules for the \enquote{happy path} of routine operations, yet those modules can be reorchestrated or overridden in exceptional cases. This design philosophy reflects an early recognition that no single process model can anticipate all future conditions, so a degree of configurability must be built in \parencite{georgakopoulosOverview1995}.

Despite efforts to introduce flexibility, traditional workflow automation faced notable challenges with \textsc{exception handling}. Exception handling refers to the ability of a system to cope with deviations, errors, or unforeseen scenarios that fall outside the predefined process flow.~\textcite{basuResearch2002} candidly observe that existing WfMS \enquote{tend to fall short whenever workflows have to accommodate exceptions to normal conditions}---i.e., when something unexpected occurs that was not explicitly modeled, the system often cannot resolve it autonomously, forcing human intervention. Typically, designers might anticipate a limited number of exception scenarios and build alternate paths for those (e.g., an approval escalation if a manager is absent). However, if a novel exception arises (say, a new regulatory requirement or an unplanned system outage affecting a step), the rigid workflow cannot handle it, and manual workarounds are needed. This problem of early workflows under dynamic conditions was widely acknowledged \parencite{georgakopoulosOverview1995}. In the pre-AI era, most workflow automation remained predominantly rule-driven and inflexible outside of predefined contingencies. Exception handling thus stood out as a critical limitation of classical automation approaches, highlighting a gap between the desire for end-to-end automation and the reality of complex, ever-changing business environments.

Another salient theme in the literature is \textsc{workflow governance}—the structures and mechanisms for overseeing automated workflows and aligning them with organizational policies. As companies entrusted core business processes to software, ensuring the correct and intended execution of those processes became vital. Key governance considerations include monitoring, auditing, and controlling workflows \parencite[cf.][p. 131]{georgakopoulosOverview1995}. A WfMS typically provides monitoring dashboards and logs so that managers can track the state and performance of process instances (e.g., to identify bottlenecks or errors). It also enforces role-based access control, ensuring that only authorized personnel perform certain tasks or approvals, which is essential for compliance in regulated industries.~\textcite{basuResearch2002} highlight the importance of organizational \enquote{metamodels} and control mechanisms that tie workflows to an enterprise's structure---for example, defining which organizational roles are responsible for each task and how escalation or overrides should happen under specific conditions. Additionally, governance extends to establishing standards and best practices for workflow design and deployment. Industry coalitions and standards bodies (such as the WFMC in the 1990s) issued reference models and interface standards to promote consistency and interoperability in workflow implementations. In the context of inter-organizational workflows, governance also means agreeing on protocols and service-level commitments between partners so that automated interactions remain trustworthy and transparent. Overall, robust governance in workflow automation ensures not only efficiency but also accountability, security, and compliance. It addresses the managerial and oversight challenges that arise once processes are no longer directly handled by individuals but by software agents following prescribed logic.

\subsection{Agentic Artificial Intelligence}\label{subsec:agentic-ai}
Agentic AI refers to systems composed of multiple interacting generative agents that autonomously collaborate to achieve complex goals. It represents a shift beyond single AI agents toward orchestrated multi-agent ecosystems, enabled largely by recent advances in the field. Whereas traditional automation (e.g.~rule-based RPA) executes predefined steps, an agentic architecture features adaptive, goal-directed agents that can perceive context, make decisions, and act with minimal hard-coded instructions \parencite[cf.][pp. 8-9]{jenningsRoadmap1998}. This idea builds on classic MAS principles of autonomy and social action \parencite{castelfranchiModelling1998,ferberMultiagent1999}, but now agents are augmented with learning and reasoning capabilities from large language models (LLMs).~\textcite{sapkotaAI2026} highlight this paradigm shift by highlightin the difference between AI agents and agentic AI, framing the later as “multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy” in pursuit of flexible problem-solving.

A defining trait of agentic AI is a higher degree of \textsc{autonomy}. Agents can operate without constant human or central control, making and executing decisions in real time. Early MAS research already emphasized agent autonomy—e.g.~agents as entities with independent control over their actions and state. Modern generative agents greatly amplify this autonomy by leveraging LLM-based reasoning to plan multi-step actions toward goals. For instance, frameworks like AutoGPT \parencite{yangAutoGPT2023} demonstrated that a single LLM-based agent can iteratively break down objectives, choose actions, and adjust based on feedback without human intervention. This autonomy promises agility and decision quality (agents can respond to situational changes or large search spaces beyond rigid scripts), but it also introduces risks. As \textcite{russellResearch2015} caution, each additional decision delegated to an opaque AI agent shifts “ethical control” away from human operators. In enterprise settings, uncontrolled autonomous decisions might lead to policy violations or unsafe actions \parencite{gauravGovernance2025}. Thus, an architectural challenge is balancing agent freedom with mechanisms to supervise or constrain critical decisions. In practice, this means designing agents with clearly scoped authorities, fail-safes, or escalation paths (e.g.~requiring human confirmation for high-impact actions) to align autonomy with organizational policies.

\textsc{tool-use capability} is another hallmark of agentic AI architectures. Agents are not limited to their built-in knowledge; they can invoke external tools, APIs, or other services as part of their reasoning loop. This extends an agent's functionality—for example, an AI agent might call a database, run a code snippet, or query web services to gather real-time information. Research shows that augmenting LLM agents with tool integration significantly improves their problem-solving scope and accuracy. Notably, the ReAct paradigm interleaves an agents chain-of-thought with tool calls, allowing it to perceive (via queries) and act (via external operations) iteratively \parencite{yaoReAct2023}. Such designs transform static LLMs into dynamic cognitive agents that can perceive, plan, and adapt---a critical capability for complex, multi-step workflows.

For workflow automation, this means an agent can not only parse instructions but also execute parts of a workflow (e.g.~trigger an RPA bot or send an alert) and then reason over the results. Architecturally, enabling tool use requires adding interface layers for the agent to safely interact with enterprise systems (APIs, databases, RPA scripts), along with policies on allowed tools. It's worth noting that tool integration adds orchestration complexity and potential error propagation paths \parencite{sapkotaAI2026}. Therefore, designs often include an orchestration layer or planner agent that manages when and how tools are invoked, checks tool outputs, and handles exceptions (e.g.~what if a tool fails or returns unexpected data). In summary, tool-use greatly enhances agent capabilities, but it demands careful architectural planning to manage the added complexity and ensure robust tool-agent interaction.

Agentic AI systems are inherently multi-agent comprising not one but many agents, often with \textsc{specialized} roles, that must \textsc{coordinate} their efforts. This multi-agent approach stems from the insight that complex workflows can be decomposed: instead of one monolithic AI agent trying to do everything, a team of agents can each handle subtasks and then combine results. Such specialization aligns with principles of OpEx (e.g.~division of labor and expertise) and has been shown to improve performance. For example, \textcite{shuEffective2024} report that a collaborative team of LLM-based agents achieved up to 70\% higher success rates on complex tasks compared to a single-agent approach.

Architecturally, coordination mechanisms are crucial to harness these gains. Agents need to communicate their intentions, share data or results, and synchronize plans. The literature distinguishes coordination structures along two dimensions: hierarchy vs.~flat and centralized vs decentralized decision-making. In a centralized hierarchical design, a top-level planner/manager agent delegates tasks to subordinate agents and integrates their outputs (akin to a project manager overseeing specialists). This can simplify global coordination and ensure alignment with a single source of truth (the planner's goal), at the cost of a single point of failure or bottleneck. Conversely, decentralized teams use peer-to-peer negotiation or voting; all agents are more equal and collectively decide on task assignments or conflict resolution (drawing on concepts from distributed AI and game theory). For instance, one recent system had developer agents jointly agree on a solution design without a central boss, mimicking consensus decision-making \parencite{qianChatDev2024}. Each approach has trade-offs: hierarchical control can be more efficient for well-structured processes (e.g., \emph{workflow automation}), while decentralized collaboration may be more robust to single-agent failure and better for ill-structured problems (e.g., research). 

Inter-agent communication protocols (what messages agents send and when) are another design facet—simple cases use direct message passing or shared memory, whereas more complex setups might use asynchronous communication. Importantly, specialization means each agent can be bounded in scope (e.g.~a compliance checker agent vs.~a data retrieval agent), which helps with scalability: each agent's LLM or reasoning module can operate within a focused context window, and different team members can even use different model types suited to their niche. This modularity and specialization, orchestrated through well-defined coordination logic, is a key architectural strength of agentic AI.~It mirrors how human organizations structure teams for efficiency, and indeed is crucial for aligning multi-agent AI workflows with complex enterprise processes. %add quotation.

As agent behaviors become more autonomous and distributed, ensuring \textsc{observability} of the system is vital. Observability here means that the internal states, decisions, and actions of agents can be monitored and understood by humans or supervisory systems. Traditional MAS literature often dealt with observability in terms of state visibility (e.g.~in partially observable environments), but in an enterprise context it translates to runtime transparency and traceability of what agents are doing and why. One challenge is that LLM-driven agents reason in natural language (or latent vectors), making their decision process somewhat opaque.~\textcite{sapkotaAI2026} highlight that AI agents “lack transparency, complicating debugging and trust”, and they advocate for robust logging and auditing pipelines to make agent operations inspectable. In practice, agentic architectures include components to log key events: each prompt an agent generates, each tool API call and its result, each decision or plan the agent commits to, etc. Such audit logs enable post- hoc analysis, error tracing, and explanations—for example, if a workflow failed or a compliance issue occurred, developers can replay the agent interactions to pinpoint the cause. Some frameworks even expose an agent's chain-of-thought (the intermediate reasoning steps) in a controlled way for debugging or compliance review.

Beyond logging, observability can be enhanced through dashboarding and alerts: e.g.~real-time monitors that track agent performance metrics or detect anomalies (like an agent taking too long on a task or generating an out-of-bounds output). The end goal is to treat an agentic AI system not as a “black box” automation, but as an observable workflow that operations teams can supervise akin to any critical IT system. This also ties into explainability: by capturing the rationale behind decisions (even if only in approximate form, such as storing the intermediate reasoning text), the system can later provide explanations for its actions, which is invaluable for trust and for continuous improvement. Overall, the literature suggests that designing for transparency—instrumenting agents with logging, and perhaps even designing agents to self-report their status—is a best practice to ensure agentic AI doesn't become an inscrutable tangle of automations. High observability supports OpEx principles by enabling traceability, accountability, and faster incident response when something goes wrong. % add quotation

Finally, a recurrent theme is the need for strong \textsc{governance} mechanisms in agentic AI architectures to ensure alignment with rules, ethics, and organizational policies. By their nature, autonomous agents may produce unexpected or undesired outcomes—a risk amplified in multi-agent settings where interactions are complex and no single agent has full oversight. Without proper governance, an agentic system could easily violate compliance requirements or strategic constraints, undermining OpEx goals (e.g.~a well-intentioned agent might inadvertently expose sensitive data or execute an unauthorized transaction). In fact, \textcite{gauravGovernance2025} warn that the \enquote{absence of scalable, decoupled governance remains a structural liability} in today's agentic AI ecosystems. To address this, researchers are exploring policy-enforcement layers that sit between the agents and the outside world. One such approach is Governance-as-a-Service (GaaS), % add quotation
 a framework that intercepts agent actions at runtime and checks them against explicit rules or constraints. Rather than trusting each agent to self-regulate, an external governance layer can block or redirect high-risk actions, log rule violations, and even adapt penalties or restrictions on agents that exhibit misbehavior over time. This effectively creates an oversight controller for the MAS—analogous to a \enquote{compliance officer} in a human organization—that ensures no single agent can compromise the system's integrity.
 
 Key design elements include declarative policy rules (defining allowable vs.~disallowed outputs or tool uses), a mechanism to monitor all agent outputs (to flag violations), and possibly a trust score or reputation model to quantify an agent's reliability based on past behavior. Beyond automated enforcement, governance also encompasses human oversight: for example, requiring human approval for certain agent decisions (human-in-the-loop checkpoints) or having a fallback where a human operator can intervene if the agents encounter an ambiguous ethical situation.
 
 In classical MAS research, analogous concepts existed like normative agents and electronic institutions that enforce “rules of engagement” among agents; the new twist is that with LLM-based agents we must often treat the models as black boxes, so governance can't be injected into their internal logic easily and must surround them instead. The overarching recommendation is that any architecture for generative multi-agent workflows should bake in governance from the start—not as an afterthought—to manage risk. As one author succinctly put it, such a system “does not teach agents ethics; it enforces them”. This governance emphasis aligns tightly with OpEx goals of compliance, risk management, and trustworthiness \parencite{gauravGovernance2025}.

In summary, the literature portrays agentic AI as an emerging paradigm for workflow automation that, if well-designed, can dramatically enhance agility, adaptability, and decision quality in operations. By combining autonomous, tool-using agents into coordinated architectures, organizations can automate complex processes that previously required human judgment. At the same time, achieving sustainable excellence with such systems demands careful attention to transparency and control: architects must ensure agents remain observable and governable to uphold compliance and reliability standards. These insights set the stage for the next section of this thesis, which will integrate OpEx principles, workflow automation requirements, and agentic AI capabilities into a unified reference architecture. The themes of autonomy, coordination, and governance identified here directly inform the design choices and requirements elaborated in the subsequent chapters.

\section{Requirements Modeling}\label{sec:mod-req}
The literature review identified recurring design concerns across operational excellence, workflow automation, and agentic AI.~Synthesizing these insights yields \emph{elicitation lists} which represent the initial outcome of requirements documentation based on systematically coded sources. A subsequent \emph{clustering} step consolidated overlapping items into a unified set of requirement candidates.

Each candidate was then reformulated into an atomic, unambiguous, and verifiable “shall” statement, following the best-practice formulation rules of \textcite{glinzHandbook2020}. The final requirements were organized into \emph{functional requirements}, \emph{quality requirements}, and \emph{constraints}, in line with the Glinz taxonomy.

Requirements were then represented in SysML~v2 as dedicated requirement elements, with their textual statements captured in the description field. Trace links connect each requirement to its source in the elicitation lists and to the architecture elements that \texttt{«satisfy»} it. This model-based representation ensures that design decisions remain traceable to validated needs and that requirement coverage can later be verified systematically. \\

\subsection{Clustering and Consolidation}
In order to derive actionable system requirements from the literature, a structured clustering process was applied. This eliminated redundancies, consolidated overlapping issues, and normalized vocabulary across disciplines. The process began by translating \emph{coded statements} from each domain into elicitation items—discrete, \emph{design-relevant units} derived from the coding units identified in the QCA process described in Section~\ref{subsec:qca}.~\\

\noindent\textsc{O --- operational excellence}
\begin{enumerate}
  \item \textsc{adaptability and agility} --- processes must remain reconfigurable in response to volatile conditions, ensuring resilience in dynamic environments.
  \item \textsc{compliance and risk management} --- regulatory adherence and transparency must be embedded into workflows to minimize compliance risks.
  \item \textsc{decision quality} --- automation should enable data-driven, timely, and well-informed decisions rather than simply increasing speed.
  \item \textsc{efficiency and continuous improvement} --- workflows should reduce manual effort, eliminate waste, and institutionalize iterative refinements.
  \item \textsc{customer-centricity} --- operations must align with user needs and service-level commitments to sustain value delivery.
  \item \textsc{user empowerment and culture} --- systems should support collaboration, transparency, and employee engagement in improvement processes.
  \item \textsc{technology integration and scalability} --- architectures must accommodate automation, AI, and cloud services to enable sustainable innovation.
\end{enumerate}

\noindent\textsc{W --- workflow automation}
\begin{enumerate}
  \item \textsc{process orchestration} --- workflow engines must enforce task sequences and business rules to guarantee reliable execution.
  \item \textsc{integration and interoperability} --- automation must seamlessly connect heterogeneous applications, data sources, and organizational boundaries.
  \item \textsc{modularity and reusability} --- workflows should be composed of modular tasks or subprocesses that can be reused and reconfigured with minimal effort.
  \item \textsc{exception handling and flexibility} --- systems must detect, manage, and escalate deviations rather than failing in unforeseen scenarios.
  \item \textsc{workflow governance} --- monitoring, audit trails, and role-based controls must ensure accountability and compliance throughout automated processes.
\end{enumerate}

\noindent\textsc{A --- agentic ai}
\begin{enumerate}
  \item \textsc{autonomy in decision-making} --- agents should operate independently within clearly scoped authority to enhance agility while managing risks.
  \item \textsc{tool use and integration} --- agents must invoke external tools, APIs, or services reliably, requiring robust interfaces and safeguards.
  \item \textsc{coordination and specialization} --- multi-agent systems should divide labor through explicit roles and structured coordination mechanisms.
  \item \textsc{observability and transparency} --- all agent actions and decisions must be logged and explainable to support trust, debugging, and compliance.
  \item \textsc{governance and compliance} --- oversight mechanisms, including policy enforcement layers and human-in-the-loop checkpoints, are essential to align agent behavior with organizational and ethical standards.
\end{enumerate}

To reduce redundancy and ensure conceptual clarity, the elicitation items from each domain were compared and consolidated based on semantic similarity and functional overlap. Particular attention was given to cross-domain intersections—such as governance appearing in both workflow automation and agentic AI—and to areas where multiple coding units aligned on a shared concern. The result of this consolidation step is a reduced set of requirement candidates, each traceable to one or more domain-specific clusters. These are summarized in Table~\ref{tab:req-clustering}, which maps the original coded clusters to the consolidated requirement areas used for formulation.

\rowcolors{3}{white}{white}
\begin{table}[H]
  \centering
  \begin{tabular}{p{2.7cm} p{5cm} p{5.5cm}}
    \toprule
    \textsc{cluster ids} & \textsc{source domains} & \textsc{consolidated~req.~area} \\
    \midrule
    \textsc{o1} & \textsc{opex} & \textsc{adaptability and agility} \\
    \textsc{o4} & \textsc{opex} & \textsc{continuous improvement} \\
    \textsc{o5} & \textsc{opex} & \textsc{customer-centricity} \\
    \textsc{o6} & \textsc{opex} & \textsc{user empowerment} \\
    \textsc{o7} & \textsc{opex}& \textsc{tech.~integration \& scal.} \\
    \rowcolor{gray!12}\textsc{o2, w5, a5} & \textsc{all~three} & \textsc{governance \& compl.} \\
    \rowcolor{gray!12}\textsc{o3, a1} & \textsc{opex,~agentic~AI} & \textsc{decision quality} \\
    \rowcolor{gray!12}\textsc{w1, w3} & \textsc{workflow~automation} & \textsc{orchest. \& modularity} \\
    \rowcolor{gray!12}\textsc{w4, a3} & \textsc{work.~aut.,~agentic~AI} & \textsc{excep.~handling \& coord.} \\
    \rowcolor{gray!12}\textsc{a4, o2} & \textsc{agentic~AI, opex} & \textsc{observability \& trac.} \\
    \rowcolor{gray!12}\textsc{w2, a2} & \textsc{work.~aut.,~agentic~AI} & \textsc{tool integration} \\
    \bottomrule
  \end{tabular}
  \caption{Mapping of domain-specific clusters to consolidated requirement areas (in-scope areas highlighted in gray).}~\label{tab:req-clustering}
\end{table}

This structured consolidation establishes a coherent foundation for refining \\ architecture-level requirements. To maintain architectural focus and avoid inflation of the requirement set, areas originating solely from managerial OpEx—namely, adaptability and agility, continuous improvement, customer-centricity, user empowerment, and technology integration and scalability—were \emph{not} modeled. Their architecturally relevant aspects were absorbed into cross-domain clusters, while non-architectural concerns were acknowledged as higher-level managerial frameworks, guidelines, and principles rather than system requirements.

\subsection{Reformulation and Classification}\label{subsec:req-clas}
The logical next step, consistent with RE methodology, is to dissect each consolidated requirement area individually and distill it into a set of atomic “shall” statements. This transformation emphasizes clarity, necessity, and verifiability, ensuring that each requirement stands alone as an actionable directive. At this stage, the focus remains on precision and alignment with design intent; formal classification into functional, quality, or constraint types follows only after this refinement process. Several requirement areas share overlapping architectural concerns. This reflects systemic interdependencies, particularly in adaptive, agent-based architectures and, more broadly, in complex systems. To avoid redundancy, each requirement is assigned to the cluster that most directly motivates it.

To address the tradeoff between best-practices and readability, a deliberate compromise was made in the formulation of requirements: strict atomic decomposition was applied selectively. In cases where overly granular formulation would hinder readability or inflate the requirement set without clear architectural benefit, semantically related concerns were consolidated into cohesive statements. Furthermore, the scope of formal requirements was restricted to those directly affecting system architecture—such as structure, behavior, and coordination—while non-architectural concerns (e.g., organizational or cultural aspects) were acknowledged in the literature but excluded from the formal specification. \\

\noindent \textsc{governance and compliance} \quad The literature consistently frames governance and compliance as architectural, not merely legal, concerns. Compliance should be embedded as a first-class constraint, with governance mechanisms preventing compliance drift as systems optimize or adapt. Classical workflow automation contributes monitoring, audit trails, and escalation, and has evolved toward policy-driven designs that decouple governance rules from core logic. %quotation
Agentic AI requires a decoupled policy layer capable of intercepting and vetoing high-risk actions, complemented by human-in-the-loop checkpoints and comprehensive auditability. Across domains, governance and compliance must operate as active, adaptable components—enforcing policy at runtime while preserving transparency and alignment with organizational and legal constraints \parencite{basuResearch2002, gauravGovernance2025}.

\begin{itemize}
  \item \textsc{[fr] policy engine}
  \item \textsc{[c] segregation of duties}
  \item \textsc{[fr] risk-based approvals and escalation}
  \item \textsc{[c] audit logging and retention}
  \item \textsc{[fr] compliance mapping and drift checks}
\end{itemize}

\noindent \textsc{decision quality} \quad The literature portrays decision quality as producing reliable, evidence-based outcomes rather than merely increasing speed. In operational excellence, this means balancing rapid execution with transparency and accountability. Classical workflow automation contributes rule enforcement, role clarity, monitoring, and escalation to standardize and audit decision points. Agentic AI extends decision reach but must be bounded and observable: agents should operate within scoped authority, log the context of their choices (prompts, tool interactions, plan commitments), and escalate high-impact actions to human review. Together, these insights imply that architecture must combine evidence aggregation with rule-based consistency, human checkpoints for risk, and traceability of decision pathways to sustain quality under change. Based on these insights, the following requirements were derived:

\begin{itemize}
  \item \textsc{[fr] evidence-based decision support}
  \item \textsc{[fr] rule-enforced decision points}
  \item \textsc{[fr] decision trace and rationale}
  \item \textsc{[fr] risk-based human approval}
  \item \textsc{[c] bounded decision autonomy}
\end{itemize}

\noindent \textsc{orchestration and modularity} \quad The literature describes classical workflow automation as the enactment of formal process models by a central engine that coordinates human and machine tasks in a defined order and enforces business rules for predictable, repeatable execution. To manage change and complexity, workflows should be composed of modular subprocesses or services that can be reused and reconfigured without redesign, often realized through service-oriented compositions. Interoperability standards and clear interfaces enable module composition across heterogeneous systems and organizational boundaries. In OpEx terms, parameterized designs allow variation without structural overhaul, preserving efficiency while enabling adaptation. Based on these insights, the following requirements were derived:

\begin{itemize}
  \item \textsc{[fr] process orchestration engine}
  \item \textsc{[c] modular process units}
  \item \textsc{[c] interface-based composition}
  \item \textsc{[fr] parameterized subprocesses}
  \item \textsc{[fr] human-machine task orchestration}
\end{itemize}

\noindent \textsc{exception handling and coordination} \quad The literature notes that classical workflow systems struggle with unforeseen exceptions; predefined alternates help, but novel cases often require human intervention. Architectures should therefore detect and route deviations to defined exception paths with appropriate escalation. In agentic AI, multi-agent designs decompose complex work into specialized roles, which demands explicit coordination structures—ranging from hierarchical planner-specialist patterns to decentralized negotiation—plus clear communication protocols (message passing or shared memory for asynchronous cases). Together, robust exception pathways and well-specified coordination ensure that workflows remain reliable when reality diverges from the “happy path,” while agent teams synchronize decisions without conflict or drift. Based on these insights, the following requirements were derived:

\begin{itemize}
  \item \textsc{[fr] exception detection and routing}
  \item \textsc{[fr] escalation to human authority}
  \item \textsc{[c] explicit coordination model}
  \item \textsc{[fr] inter-agent communication protocol}
  \item \textsc{[fr] tool failure handling in exceptions}
  \item \textsc{[fr] conflict resolution}
\end{itemize}

\noindent \textsc{observability and traceability} \quad The literature emphasizes that enterprise automation must be inspectable at runtime and explainable post hoc. Classical workflow systems contribute monitoring and logs for process execution visibility. Agentic AI adds opacity risks; to mitigate them, architectures should log agent prompts, tool interactions, and plan/decision commitments, expose dashboards and alerts for anomaly detection, and support replay so that failures and outcomes can be reconstructed and explained. Across domains, observability and traceability provide accountability, faster incident response, and the basis for explaining why a particular decision or action occurred. Based on these insights, the following requirements were derived:

\begin{itemize}
  \item \textsc{[fr] event logging pipeline}
  \item \textsc{[fr] decision trace and rationale}
  \item \textsc{[fr] dashboards and alerts}
  \item \textsc{[fr] replay for post-hoc analysis}
  \item \textsc{[c] agent status self-reporting}
\end{itemize}

\noindent \textsc{tool integration} \quad The literature presents integration as foundational to both classical workflow automation and agentic AI.~Workflow systems must connect heterogeneous applications and data sources, often across organizational boundaries, using clear interfaces and interoperability standards. Agentic AI adds tool-use: agents invoke enterprise APIs, databases, or RPA scripts through interface layers, while an orchestration component manages when and how tools are called and checks results. Architecturally, tool integration therefore centers on well-defined adapters and contracts, protocol-level interoperability, and data transformation to enable end-to-end workflows without brittle coupling. Based on these insights, the following requirements were derived:

\begin{itemize}
  \item \textsc{[fr] integration connectors}
  \item \textsc{[fr] agent tool adapters}
  \item \textsc{[c] interface contracts and schemas}
  \item \textsc{[fr] inter-organizational interoperability}
  \item \textsc{[fr] data transformation layer}
  \item \textsc{[c] invocation safeguards}
\end{itemize} \\

The requirements were then consolidated into a final list, grouped by class---\emph{functional requirements} and \emph{constraints}---to provide a single, unambiguous baseline for the subsequent SysML formalization. Here an example of each requirement class: \\

\noindent \textsc{FR-09 policy engine} \quad The system shall provide a decoupled policy evaluation component that can validate, veto, or redirect workflow executions and agent actions at runtime. \\

\noindent \textsc{C-03 bounded decision autonomy} \quad The system shall constrain agent decision-making to clearly scoped authority levels aligned with organizational objectives. \\

The complete list of requirements can be found in Appendix~\ref{app:req-list}. As the reader can see, \emph{quality requirements} were left out of scope at this stage because this thesis focuses on specifying \emph{what} the system must do and \emph{which} governance and operational constraints it is subject to. Moreover, quality attributes vary significantly by deployment context and would require domain-specific targets, which remain out of scope here. Some quality-related concerns (e.g., auditability, interoperability) were operationalized as \emph{constraints} rather than standalone quality requirements. \\

While the list optimizes precision, it is not yet visually informative nor operationally easy to manage. In the next section, the requirements are recast as SysML \texttt{«requirement»} elements and diagrams to improve visibility, traceability, and change control—linking them explicitly to their sources and preparing \texttt{satisfy}/\texttt{verify} relations for the architectural and evaluation work that follows.

\subsection{Model-Based Representation}\label{subsec:req-model}
The requirements were encoded into a SysML~v2 model that serves as the central design artifact for the remainder of this work. Concretely, each refined \enquote{shall} statement is represented as a dedicated SysML \texttt{<<requirement>>} element whose description preserves the validated text verbatim. To make the model auditable and operable, each element carries lightweight metadata (stable identifier, short title, source, and cluster) and is \emph{anchored} to the system-of-interest via the element's \emph{subject}. This anchoring establishes the scope of each requirement without committing to concrete architectural structure at this stage. For navigability and later analysis, requirements are organized into subpackages that mirror the six clusters consolidated in §\ref{subsec:req-clas} (governance and compliance; decision quality; orchestration and modularity; exception handling and coordination; observability and traceability; tool integration). The model further \emph{prepares} standard SysML relations (\texttt{<<satisfy>>}, \texttt{<<verify>>}) that will be instantiated in §\ref{sec:mod-mas} once architectural elements are introduced; this sequencing preserves methodological clarity while ensuring traceability is planned from the outset. Taken together, the requirement model forms a coherent, machine-interpretable blueprint that unifies functional directives and constraints in one representation and supports systematic evolution throughout the design process \parencite{IEEEStandard1990}.

This subsection focuses exclusively on how the requirements are represented; architectural structure and behavior are treated in §\ref{sec:mod-mas}. The following decisions govern the model:
\begin{itemize}
  \item \emph{Requirement element and metadata.} Each validated statement is captured as a SysML \texttt{<<requirement>>} with: (i) a stable ID matching the consolidation in §\ref{subsec:req-clas} (e.g., \texttt{FR-01}, \texttt{C-03}), (ii) a short title to aid readability, (iii) the original text in the description (self-documenting model), (iv) a \texttt{source} tag referencing the consolidation artifact, and (v) a \texttt{cluster} tag indicating its organizational grouping. This keeps the model faithful to the source while enabling queries and views.
  \item \emph{Subject anchoring.} Every requirement has a \emph{subject} set to the system-of-interest. The subject establishes what the requirement constrains or obligates, while deliberately avoiding bindings to specific blocks or interfaces before the architecture is introduced. This prevents premature design commitments while keeping semantics precise.
  \item \emph{Granularity and verifiability.} Composite statements are decomposed until each requirement is atomic, necessary, and verifiable. Where a statement mixes an obligation with a performance bound or policy constraint, it is split into a functional requirement (the obligation) and one or more constraint requirements (the bounds). This respects the consolidated content yet ensures each element is testable.
  \item \emph{Functional vs.\ constraint tagging.} The model distinguishes requirement \emph{intent} via a lightweight tag: \emph{Functional} for obligations on behavior/capability; \emph{Constraint} for limits, policies, or non-functional bounds. This tagging supports later viewpoints (e.g., verification planning) without introducing parallel taxonomies.
  \item \emph{Derivation and refinement.} When a consolidated statement is systematically specialized (e.g., split by scenario or mode), the resulting elements are connected with \texttt{derive} or \texttt{refine} relationships. These links are intra-requirement only at this stage; links to design are deferred to §\ref{sec:mod-mas}.
  \item \emph{Verification planning metadata.} Each requirement optionally records a \emph{planned} verification method (Analysis, Inspection, Test, or Demonstration) as metadata. The corresponding \texttt{<<verify>>} links are created in §\ref{sec:mod-mas} when test or analysis artifacts exist, keeping §\ref{subsec:req-model} strictly about requirement representation while making verification intent explicit \parencite{IEEEStandard1990}.
  \item \emph{Organization by cluster.} Requirements are grouped into six subpackages, one per cluster from §\ref{subsec:req-clas}. Cross-cutting concerns (e.g., governance or observability) remain in their home cluster but may later be \emph{traced} to multiple architectural elements; organizing by cluster improves readability without pre-judging component boundaries.
  \item \emph{Change provenance.} Each requirement preserves a brief rationale note (when relevant) and the source reference to the consolidation step. This enables impact analysis: a change in a requirement's text or status can be traced back to the originating consolidation item.
\end{itemize}

The following textual snippet shows how a consolidated statement becomes a model element while remaining architecture-agnostic:
\begin{footnotesize}
  \begin{verbatim}
  requirement def FR_01_ProcessOrchestrationEngine {
    attribute id     = "FR-01";
    attribute title  = "Process orchestration engine";
    attribute cluster= "OrchestrationAndModularity";
    attribute source = "Consolidation §4.2 / Appendix A.1";
    subject soi : SystemOfInterest;
    doc /* The system shall execute workflow models by dispatching tasks
          to human or software actors according to model control flow
          and business rules. */
    // Trace and verification links are prepared here, and instantiated in §\ref{sec:mod-mas}.
  }
  \end{verbatim}
\end{footnotesize}
This form preserves the validated text, records minimal yet sufficient metadata, and binds the requirement to the system-of-interest. Additional derived or refined elements (e.g., for parameterized subprocesses or exception routing) are represented similarly and related via \texttt{derive}/\texttt{refine} within the requirement package.

The model establishes \emph{where} trace links will exist and \emph{how} they will be expressed, but it intentionally defers instantiation of \texttt{<<satisfy>>} and \texttt{<<verify>>} until §\ref{sec:mod-mas} introduces architectural and verification artifacts. This preserves a clean separation of concerns while aligning with IEEE's notion of traceability as the systematic linkage of requirements, design, and verification artifacts \parencite{IEEEStandard1990}. Once §\ref{sec:mod-mas} defines architectural elements, each requirement receives explicit \texttt{<<satisfy>>} links from the corresponding structural or behavioral elements, and verification cases attach via \texttt{<<verify>>} according to the planned methods.

Representing the requirements in SysML~v2 yields three immediate benefits: (i) it consolidates all validated needs into a single, analyzable model with explicit scope and provenance; (ii) it enforces conceptual integrity by keeping terminology and interfaces consistent in one place and enables early review against the requirements (e.g., checking that exception scenarios can be given handling paths once behavior is introduced); and (iii) it creates a stable basis for subsequent instantiation, allowing the model to guide development as a living blueprint \parencite{peffersDesign2007}. These outcomes support the design-science contribution while remaining faithful to the constraint that architecture-specific content is presented in §\ref{sec:mod-mas}.

\medskip
The complete SysML~v2 requirement package (textual listing) is provided in Appendix~\ref{app:req-mod}.

\section{Architecture Modeling}\label{sec:mod-mas}
This section translated the validated requirement model from §4 into an architectural description suitable for design, analysis, and later verification. To preserve separation of concerns, the representation choices of §\ref{subsec:req-model} were taken as given; instantiation of trace links (\texttt{<<satisfy>>}, \texttt{<<verify>>}) and the introduction of concrete design elements were deliberately performed here. The architecture was kept technology-agnostic and context-aware: it respected legacy coexistence, policy governance, and auditability-by-design while avoiding premature selection of platforms or products.

The section was organized to make the path to the result explicit. First, the scope, assumptions, and architectural drivers were derived from the requirement clusters, and the modeling viewpoints were fixed to address those drivers systematically (§\ref{subsec\:arch-drivers}). Next, the system was decomposed into structural elements and interfaces that could realize those concerns without overcommitting the solution space (§5.2). Finally, the behavioral coordination of agents, humans, and external services was described, including exception paths and the concretization of traceability from requirements to design and verification artifacts (§5.3). This progression ensured that each architectural decision remained justified by a validated need and that coverage could be inspected end-to-end \parencite{IEEEStandard1990,peffersDesign2007}.

\subsection{Architectural Drivers and Viewpoints}\label{subsec\:arch-drivers}
Before introducing concrete structures and behaviors, the scope of the architecture was first defined, the principal assumptions and constraints were recorded, and the architectural drivers were derived from the requirement clusters in §\ref{subsec:req-clas}-§\ref{subsec:req-model} (see Appendix~\ref{app:req-list} for the complete list). On this basis, the modeling viewpoints to be used in §\ref{sec:mod-mas} were selected so that the path from validated needs to design decisions remained explicit and inspectable \parencite{peffersDesign2007}.

The architecture targeted a conceptual, technology-agnostic multi-agent system (MAS) for enterprise workflow automation. The \emph{system-of-interest} comprised: (i) an orchestration core coordinating human and software tasks, (ii) specialized agents contributing analysis, drafting, or review capabilities, (iii) a governance layer enforcing policies and approvals, (iv) an integration layer connecting external tools and data sources, and (v) observability mechanisms for runtime inspection and audit. Organizational processes, cultural programs, or purely managerial OpEx initiatives were treated as context; only their architecturally relevant aspects (e.g., policies constraining runtime behavior) were carried into the design scope.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\linewidth]{ressources/MAS/figures/5_1a_scope_boundary.png}
  \caption{The system-of-interest was bounded with respect to human actors and legacy systems, highlighting the orchestration core, specialized agents, governance, integration, and observability.}
  \label{fig:5.1a-scope-boundary}
\end{figure}

The following design assumptions and boundary conditions governed subsequent choices:
\begin{itemize}
\item \emph{Legacy coexistence.} Existing systems (WfMS, data stores, line-of-business applications) remained in place; the architecture had to integrate without requiring systemic replacement.
\item \emph{Technology neutrality.} No concrete product stack was prescribed in §\ref{sec:mod-mas}. The description remained at the level of components, roles, and interfaces so that multiple implementations would remain feasible.
\item \emph{Governed autonomy.} Agents operated under explicit policies and human oversight; high-risk actions required approvals. This constraint was treated as first-class rather than an afterthought.
\item \emph{Auditability by design.} All material actions (workflow transitions, tool invocations, agent decisions) were required to be observable and reconstructable for verification and compliance purposes \parencite{IEEEStandard1990}.
\item \emph{Interface contracts.} External tool access occurred through well-defined adapters with error handling and fallbacks; direct, ad hoc coupling to third-party APIs was excluded.
\item \emph{Performance envelopes.} Only coarse bounds (e.g., end-to-end latency budgets sufficient for human-in-the-loop work) were imposed in §\ref{sec:mod-mas}. Quantitative tuning was deferred to implementation-specific work.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{ressources/MAS/figures/5_1d_assumptions_mindmap.png}
  \caption{The principal assumptions and constraints framed subsequent architectural choices (legacy coexistence, technology neutrality, governed autonomy, auditability by design, interface contracts, and coarse performance envelopes).}
  \label{fig:5.1d-assumptions}
\end{figure}

Consistent with the consolidated requirement clusters, the following concerns drove the architecture and framed trade-offs:
\begin{itemize}
\item \emph{Governance and Compliance.} Policy enforcement, approval gates, and end-to-end audit trails were required to be embedded into workflow and agent interactions, not bolted on later (Cluster: Governance\&Compliance).
\item \emph{Decision Quality.} Decisions produced or assisted by agents had to be explainable and reviewable, with recorded inputs, rationales, and thresholds for escalation (Cluster: DecisionQuality).
\item \emph{Orchestration and Modularity.} Processes were to be composed from reusable units (subprocesses, services, agent capabilities) coordinated by an orchestration core, enabling substitution and incremental extension (Cluster: Orchestration\&Modularity).
\item \emph{Exception Handling and Coordination.} Defined patterns for anomaly detection, recovery, and human escalation were required; multi-agent coordination had to avoid deadlocks and ensure progress (Cluster: ExceptionHandling\&Coordination).
\item \emph{Observability and Traceability.} Uniform logging of states, prompts/tool calls, and workflow transitions was mandated, enabling replay, monitoring, and audit (Cluster: Observability\&Traceability).
\item \emph{Tool Integration.} Access to heterogeneous external systems occurred via adapters with contracts for data shape, error semantics, and timeouts (Cluster: ToolIntegration).
\end{itemize}
These drivers operationalized the validated requirements into concrete concerns to be satisfied by the architecture. They also provided the anchor for trace links from requirements to design artifacts and later verification items \parencite{IEEEStandard1990}.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\linewidth]{ressources/MAS/figures/5_1b_drivers_overview.png}
  \caption{The six architectural drivers were derived from the consolidated clusters in §4 and were used to steer design decisions.}
  \label{fig:5.1b-drivers-overview}
\end{figure}

To keep the description focused and consistent with §\ref{subsec:req-model}, four complementary SysML viewpoints were chosen. Each viewpoint addressed specific driver concerns and prepared traceability for §5.2-§5.3:
\begin{itemize}
\item \emph{Requirements/Traceability View.} The requirement elements from §\ref{subsec:req-model} (with IDs and cluster tags) were taken as the canonical source and linked to architectural elements via \texttt{<<satisfy>>} once those elements were introduced in §5.2. This view ensured that governance, decision-quality, and observability constraints were explicitly mapped rather than assumed \parencite{IEEEStandard1990}.
\item \emph{Structural View.} Using block-definition and internal-block diagrams, the static composition (orchestrator, agent roles, governance services, adapters, data stores) and their interfaces were specified. This view made modularity and integration decisions explicit and provided the loci for \texttt{<<satisfy>>} links in §5.2.
\item \emph{Behavioral View.} Activity and sequence diagrams captured coordination patterns, nominal flows, and exception paths (including human-in-the-loop checkpoints). This view instantiated decision-quality and exception-handling drivers and prepared \texttt{<<verify>>} hooks for test scenarios in §5.3.
\item \emph{Parametric/Constraints View.} Where relevant, policy and performance envelopes were represented as constraints tied to structural or behavioral elements (e.g., approval thresholds, timeout policies). Detailed quantitative models were deliberately deferred; only architecturally binding constraints were recorded here.
\end{itemize}

By defining scope and assumptions up front, deriving drivers from §4's clusters, and selecting viewpoints aligned to those drivers, the basis for §5.2 (structural decomposition and interfaces) and §5.3 (behavioral coordination and traceability) was established. The organization ensured that each design decision could be traced back to a specific driver and, ultimately, to a validated requirement in Appendix~\ref{app:req-list}, thereby preserving the separation of concerns introduced in §\ref{subsec:req-model} while enabling systematic instantiation in §\ref{sec:mod-mas} \parencite{IEEEStandard1990,peffersDesign2007}.

\subsection{Structural Decomposition and Interfaces}\label{subsec:structural-decomposition}
Guided by the drivers in §\ref{subsec:arch-drivers} and the validated requirements in §\ref{subsec:req-model} (Appendix~A.1), the \emph{system-of-interest} was instantiated as a layered multi-agent system. The structure comprised: (i) an orchestration core coordinating role-specialized agents, (ii) a governance/policy service enforcing approvals and guardrails, (iii) an integration/adapter layer encapsulating access to external systems, and (iv) observability components for audit and replay. Each element was introduced with explicit interface contracts so that the path from requirements to design decisions remained inspectable and traceable via \texttt{<<satisfy>>} links \parencite{IEEEStandard1990}.

\emph{Orchestration core.} The \emph{orchestrator} acted as control plane for task decomposition, assignment, and aggregation. It maintained workflow state, routed \emph{TaskSpec} objects to agents, coordinated human-in-the-loop checkpoints, and consulted the governance service prior to high-risk actions. The orchestrator also accessed shared adapters where cross-cutting lookups were required. This element provided the structural locus for mapping orchestration and exception-handling requirements to concrete design artifacts.

\emph{Agent roles.} Role-specialized agents (\emph{planner}, \emph{researcher}, \emph{analyst}, \emph{reviewer}, \emph{writer}) encapsulated distinct capabilities. Each agent implemented a uniform task port and produced typed \emph{TaskResult} outputs. This separation supported modular substitution and progressive refinement of capabilities without affecting the orchestrator or other layers. Where peer collaboration was beneficial, controlled agent-to-agent exchanges were permitted using the same message schema to avoid ad hoc couplings.

\emph{Governance/policy service.} The \emph{compliance} service externalized policies, approvals, and guardrails. Agents and the orchestrator performed policy checks before executing sensitive operations and recorded decisions for audit. Treating governance as a first-class service ensured consistent enforcement across the MAS and enabled explicit trace links from governance requirements to concrete policy checks \parencite{IEEEStandard1990}.

\emph{Integration/adapter layer.} The \emph{integrations} package (\emph{mcp}, \emph{a2a}, \emph{toolProxy}) encapsulated access to heterogeneous backends through stable contracts. Adapters exposed named \emph{operations} with declared parameter/response shapes and error semantics. Agents invoked tools only through these adapters; direct coupling to third-party APIs was excluded by design. This layer localized variability of external systems and preserved technology neutrality set in §\ref{subsec:arch-drivers}.

\emph{External systems and observability.} External data stores, line-of-business applications, and services were represented as environment elements behind adapters. The \emph{observer} and \emph{auditLog} components captured material actions (workflow transitions, tool invocations, policy outcomes) to enable replay, inspection, and verification in §5.3, thereby operationalizing auditability-by-design \parencite{IEEEStandard1990}.

\emph{Interface contracts.} To make composition explicit and to support later verification, the following contracts governed interactions across layers. Each contract was specified with message names, payload fields, and error semantics (timeouts, retries, compensations), and was realized as provided/required ports in the structural view:
\begin{itemize}
  \item \emph{Task interface (orchestrator\textendash agent).} The orchestrator provided \texttt{assignTask(taskSpec)} and \texttt{getResult(taskId)}; each agent required these and returned a typed \emph{TaskResult}. Idempotency keys and correlation identifiers were mandatory to ensure replay safety.
  \item \emph{Peer interface (agent\textendash agent).} Where permitted, agents exchanged \texttt{requestInfo(query)} and \texttt{provideInfo(data)} using the same schema as the task interface. Policy tags were attached so that governance checks could be enforced uniformly.
  \item \emph{Policy service (client\textendash compliance).} Clients issued \texttt{checkPolicy(policyId, context)} and recorded decisions via \texttt{logEvent(event)}. Negative decisions included explanatory rationales to support decision quality and later review.
  \item \emph{Tool invocation (client\textendash adapter).} Clients called \texttt{invokeTool(toolId, params)} and received \texttt{ToolResult} with standardized success/failure envelopes. Adapters declared timeout budgets and compensations for partial failures.
  \item \emph{Observability sink (producer\textendash observer/auditLog).} Producers emitted \texttt{emit(event)} for state transitions, prompts, tool calls, and policy outcomes. Event schemas included timestamps, actor identifiers, and integrity digests to support verification.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\linewidth]{ressources/MAS/figures/5_2_struct_overview.png}
  \caption{Composite structural view and contracts. The \emph{orchestrator} provided a uniform task interface to role-specialized agents; agents and orchestrator invoked tools via adapters; policy checks were routed to \emph{compliance}; and material events were emitted to \emph{observer}/\emph{auditLog}. External systems remained reachable only behind adapters.}
  \label{fig:5.2-struct-overview}
\end{figure}

\subsection{Behavioral Coordination and Verification}\label{subsec:behav}
This section realizes the structural design from §5.2 by defining the dynamic workflow of agent interactions and the artifacts used for verification. A central \emph{orchestrator} supervises the process, dispatching work to specialized agents (planner, researcher, analyst, writer, reviewer) and enforcing governance. Each agent executes a clearly-scoped task via the \texttt{Task} interface, while the orchestrator invokes external tools only through the \texttt{ToolAdapter} and checks every critical action via the \texttt{PolicyService}. For example, the orchestrator sequences tasks and treats sub-agents as “tools”, and a decoupled policy engine can validate or veto actions at runtime. All agent actions and decisions emit structured events to the \texttt{ObservabilitySink} (observer/auditLog channels) to build an execution trace. This coordination supports the architecture drivers (e.g., Governance&Compliance through policy gates and audit logging; Orchestration&Modularity via clear task handoffs) and produces artifacts (logs, traces) that tie back to requirements for verification. In the nominal case, the orchestrator begins by receiving a top-level task and decomposes it into subtasks. It assigns these in sequence: first the planner, then the researcher, followed by the analyst, writer, and finally the reviewer. Each agent performs its duty and returns results to the orchestrator via \texttt{Task}. External services or tool calls (e.g. via \texttt{mcp} or \texttt{a2a} interfaces or a \texttt{toolProxy}) are invoked only through the \texttt{ToolAdapter}, ensuring uniform request/response handling. Before executing any step that modifies data or affects outcomes, the orchestrator calls the \texttt{PolicyService}; the policy engine can approve the action or veto/redirect it according to rules. At each step, the system emits a timestamped event to the \texttt{ObservabilitySink}, building an audit log of decisions and tool invocations. This nominal coordination pattern (illustrated in Figure \ref{fig:5.3-nominal}) ensures that each agent’s role is clear, all tool uses are mediated by adapters, and governance checks and observability are applied at defined gates. \begin{figure}[t]
\centering
\includegraphics[width=0.5\linewidth]{ressources/MAS/figures/5_3_behavior_nominal.png}
\caption{Nominal behavioral coordination. The \emph{orchestrator} delegated tasks to role-specialized agents, invoked tools exclusively via adapters, enforced policy checks at defined gates, and emitted material events to \emph{observer}/\emph{auditLog}.}
\label{fig:5.3-nominal}
\end{figure} The architecture also addresses concurrency and synchronization. To improve throughput, independent subtasks (especially data-retrieval or “reading” tasks) can be executed by spawning multiple agents in parallel under orchestrator control. However, the design avoids uncontrolled parallelism: all tasks carry a unique correlation ID via the \texttt{Task} interface, enabling matching of responses and idempotent retries. For operations that produce shared outputs, parallel execution is carefully managed. In particular, parallel “write” or content-generation tasks are serialized or merged by a dedicated integrator agent to prevent inconsistency. In general, the system favors a breadth-then-depth strategy: multiple agents may gather information in parallel, but final synthesis or authoring is done by a single agent under the orchestrator’s coordination. A shared context (e.g. a blackboard) holds intermediate results so that each agent sees the relevant state. These measures – task correlation via unique IDs, adapter-level idempotency, and controlled merging of results – ensure consistent state across agents while allowing safe parallelism. \begin{itemize}
\item \emph{Timeouts and retries.} The \texttt{ToolAdapter} enforces timeouts on external calls and automatically retries them as needed (using idempotency keys), recording each retry or failure event to the auditLog.
\item \emph{Policy denial with rationale.} If a planned action is disallowed by the PolicyService, the orchestrator aborts it and emits an audit event that includes the policy rule or justification for denial.
\item \emph{Low-confidence escalation.} The orchestrator detects low-confidence or undefined scenarios (fail-safe conditions) and escalates these cases to a human supervisor or higher authority. Such escalations are logged, and automated agents pause until resolution.
\item \emph{Conflict resolution.} When agents’ outputs or decisions conflict, the orchestrator invokes a resolution mechanism (e.g. replanning or consensus procedures) in accordance with the conflict-resolution requirements. The outcome and any agent negotiations are logged.
\item \emph{Compensation for partial failure.} If an agent or tool invocation fails persistently (even after retries), the system triggers predefined compensation or recovery actions (as specified by exception subprocesses). Recovery steps (such as invoking alternative tools or notifying humans) are recorded in the audit trail.
\end{itemize} The verification of this behavioral model is achieved via explicit links between behaviors and requirements. Each interaction scenario is tagged with relevant requirement IDs from Appendix A.1 (e.g. FR-09, FR-10 for policy checks, FR-07/FR-08 for exception handling). UML <<verify>> relations connect sequence diagrams to these requirements, and test cases use the auditLog as an oracle to check expected event sequences and outputs. For example, an acceptance test might replay a task sequence and assert that denied actions produce a corresponding “policy denied” event and no unauthorized changes. Message schemas and policy decisions are validated against the defined interface contracts and policy rules, and performance bounds (response times, throughput) are checked against the envelopes from §5.1. This approach follows standard verification practices (e.g. IEEE software engineering definitions), ensuring that each behavior satisfies its design constraints and acceptance criteria. Each element of the coordination behavior maps back to the architecture’s requirement clusters. Policy enforcement and audit logging fulfill the Governance&Compliance and Observability&Traceability drivers (audit trails and human checkpoints), structured task delegation implements Orchestration&Modularity, error paths address ExceptionHandling&Coordination, and tool-adapter patterns satisfy ToolIntegration. In this way, every behavioral flow can be traced to the corresponding requirement cluster (e.g. governance checks to Governance&Compliance, decision trace logging to Observability&Traceability).

% \section{Discussion: Applicability Criteria}\label{sec:discussion}

% \section{Conclusion}\label{sec:conclussion}

%Future work should extend this conceptual design into practical evaluation and implementation. In particular, empirical validation of the architecture in industry settings, tool-supported instantiation in SysML, and comparative studies against traditional workflow automation would provide valuable evidence of its applicability and impact. Further, integrating additional agentic AI capabilities such as autonomous negotiation or explainability could enhance both usability and compliance assurance.