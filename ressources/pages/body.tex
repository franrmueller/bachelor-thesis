\section{Introduction}\label{sec:intro} % finished
Organizations across industries continue to face persistent challenges in achieving operational excellence (OpEx). Fragmented processes, manual interventions, and inconsistent data quality undermine efficiency and decision-making. Legacy workflows and siloed systems exacerbate these inefficiencies, while traditional automation approaches often lack the adaptability needed in dynamic business environments. For companies, this translates into slower response times, higher compliance risks, and limited scalability—issues that directly threaten competitiveness.

Agentic AI, building on the advances of generative artificial intelligence (GenAI), opens new possibilities to extend automation beyond deterministic scripts. While GenAI provides the cognitive and generative capabilities, agentic AI leverages these to create adaptive, tool-using agents that can plan, act, and coordinate---thereby supporting governance, decision quality, and organizational agility. Despite this potential, both practice and academic literature lack structured strategies and conceptual frameworks for embedding such agentic capabilities into operational workflows in a scalable and value-driven way. This gap motivates the present research.

In this context, multi-agent systems (MAS) can serve as a reference architecture for integrating GenAI-enabled agentic AI into enterprise workflow automation. The central research question is:

\vspace{0.5\baselineskip}
\emph{How can a MAS architecture be designed to integrate GenAI capabilities into workflow automation, in order to enhance agility, compliance, \& decision quality to achieve OpEx?}
\vspace{0.5\baselineskip}

To answer this question, the study addresses the following sub-questions:
\begin{itemize}
    \item \emph{Which design requirements are necessary to align a multi-agent architecture with the goals of OpEx?}
    \item \emph{How should a MAS be architected to fulfill these requirements?}
    \item \emph{Under which conditions is deploying a generative multi-agent architecture justified over traditional automation approaches?}
\end{itemize}

Methodologically, the thesis applies Design Science Research (DSR) to develop a conceptual reference architecture. The approach synthesizes requirements from academic literature and OpEx principles, models agent roles and interactions, and derives applicability conditions for real-world deployment.
Although the architecture is designed to remain industry-agnostic, a use case from the financial services sector is introduced to illustrate how the conceptual model can be instantiated in a regulated, legacy-intensive environment.

The core contribution of this work is a conceptual design of a MAS that leverages GenAI to support OpEx in enterprise workflows. Specifically, it delivers:
\begin{itemize}
    \item \emph{A structured synthesis of system requirements derived from academic literature and OpEx principles.}
    \item \emph{A conceptual architecture detailing agent roles, interactions, and integration points.}
    \item \emph{A set of applicability conditions and design considerations to guide future deployment and evaluation of generative multi-agent architectures in practice.}
\end{itemize}

The scope is limited to conceptual design; formal evaluation and technical implementation are proposed as future work. Although the architecture is designed to remain industry-agnostic, a use case from the financial services sector is introduced to illustrate how the conceptual model can be instantiated in a regulated, legacy-intensive environment.

After this introduction in Section~\ref{sec:intro}, the thesis is structured as follows: Section~\ref{sec:method} outlines the research methodology, including the use of Design Science Research (DSR) and supporting methods. Section~\ref{sec:lit-rev} presents a literature review on operational excellence, workflow automation, and agentic AI.~Section~\ref{sec:mod-req} details the synthesis and modeling of requirements. Section~\ref{sec:mod-mas} develops the conceptual multi-agent architecture. Section~\ref{sec:discussion} discusses the applicability of MAS in workflow automation use cases, and Section~\ref{sec:conclussion} concludes with reflections and directions for future research.

\section{Methodology}\label{sec:method}
This thesis applies DSR methodology to create a conceptual artifact---a multi-agent architecture for workflow automation. Practically, the approach unfolded in three steps: (1) \emph{reviewing the literature} on OpEx, workflow automation, and agentic AI;~(2) \emph{deriving and structuring requirements} from literature and case material into a requirements model; and (3) \emph{designing a conceptual system architecture} using Systems Modeling Language (SysML).

Supporting methods included Mayring-style qualitative content analysis (QCA) for the review, requirements engineering (RE) and systems analysis for the requirements model, and information systems design (ISD) to structure the architecture and ensure requirement-to-design traceability, supported by SysML modeling practices from Model-Based Systems Engineering (MBSE). Within DSR, the work focuses on problem identification, objective definition, and conceptual design, while instantiation/demonstration and formal evaluation are out of scope given the bachelor-thesis format and resource constraints. This scoping maintains methodological rigor while keeping the contribution focused: a well-argued reference architecture ready for subsequent implementation and empirical evaluation.

\subsection{Qualitative Content Analysis}\label{subsec:qca}
To ensure a structured literature review, this thesis employed qualitative content analysis following \textcite{mayringQualitative2022}. QCA offers a transparent, rule-based procedure for synthesizing knowledge from textual sources while retaining interpretative depth. In this work it supports the DSR process \parencite{peffersDesign2007} within the problem identification and motivation phase, where the aim is to understand the state of the problem domain and justify the value of a solution. Following Mayring, the analytical framework was defined prior to coding:
\begin{itemize}
    \item \textit{Analysis unit}: the overall literature corpus addressing OpEx, workflow automation, and agentic AI.\@
    \item \textit{Context unit}: individual publications (books, peer-reviewed articles, standards, industry reports, conference transcripts, and case studies).
    \item \textit{Coding unit}: discrete statements or conceptual claims relevant to the intersection of OpEx, automation paradigms, and AI-based MAS.
\end{itemize}

A \emph{mixed deductive-inductive} approach was used. Deductive categories were derived from established theory, including OpEx dimensions (adaptability, compliance, decision quality) and prior automation frameworks (robot process automation, RPA;~intelligent process automation, IPA). Inductive categories were generated from the material itself, capturing emerging issues such as ``guardrails,'' ``observability,'' and ``traceability'' in agentic AI systems.  Coding followed Mayring's rule-governed categorization to ensure consistency and avoid arbitrary interpretation. The resulting categories served two functions:
\begin{itemize}
    \item \textit{Problem representation}: categories structured how the research problem was represented, aligning with \textcite{hevnerDesign2004}, who emphasize that effective constructs are essential to problem framing.
    \item \textit{Derivation of objectives}: categories were transformed into metarequirements that guided the definition of solution objectives in Activity~2 of the DSR methodology \parencite{peffersDesign2007}.
\end{itemize}

The review was conducted by systematically coding the literature across the three pillars (OpEx, workflow automation, MAS). 
For example, claims such as ``RPA is brittle under interface changes'' were coded under the deductive category \emph{limitations of RPA}, 
while repeated references to audit trails and logging practices were inductively grouped under \emph{traceability}. 
For each publication, relevant statements were assigned to categories using predefined coding rules. 
The resulting category set both structures Section~\ref{sec:lit-rev} and forms the basis for RE in Section~\ref{subsec:re-sa}. Additional categories such as efficiency, customer-centricity, and user empowerment emerged inductively during coding; these are elaborated in Section~\ref{subsec:op-ex}.

\subsection{Requirements Engineering \& Systems Analysis}\label{subsec:re-sa} % finished
Following the \textcite{IEEEStandard1990} definition, a requirement is a \emph{condition or capability needed by a user to solve a problem or achieve an objective}. RE provides the systematic means to derive such objectives. In this thesis, RE was applied in the early phases to ensure that the conceptual architecture rests on precise, validated needs rather than general aspirations.

The synthesis of requirements followed a structured but literature-driven process. Recurring design concerns were identified from the results of the qualitative content analysis (QCA), documented in \emph{elicitation lists}, and then consolidated into a unified set of requirement candidates. Consistent with \textcite{glinzHandbook2020}, each candidate was reformulated into an atomic, unambiguous, and verifiable “shall” statement and classified into \emph{functional requirements} (system behaviors), \emph{quality requirements} (non-functional attributes such as performance or compliance), or \emph{constraints} (technological or regulatory limits).

While this procedure draws on the phases described by \textcite{herrmannGrundlagen2022} (elicitation, documentation, analysis, management), it was adapted to the scope of this thesis: instead of stakeholder workshops, the primary elicitation source was the systematically coded literature. To situate the requirements, a complementary systems analysis defined the system boundary, identified stakeholders and external actors, and clarified interface obligations---helping prevent scope creep and omissions.

Requirements were then represented in SysML requirement diagrams.~Trace links connect each documented requirement to the respective architecture elements, enabling full requirement-to-design traceability via \emph{«satisfy»} and \emph{«verify»} relationships. This model-based approach ensures that design decisions can always be traced back to validated needs and that no requirement was overlooked.

In summary, the integration of RE and systems analysis provided a structured, traceable, and quality-assured requirement set. This foundation anchors the subsequent conceptual architecture in rigorously defined objectives, ensuring consistency with both DSR methodology and operational excellence goals.

\subsection{Information Systems Design}\label{subsec:isd}
In line with DSR, this thesis applies principles of information systems design to structure the artifact. The architecture was modeled in SysML, making use of MBSE practices to ensure requirement-to-design traceability. While originating in MBSE originates in systems engineering, its modeling discipline is transferable to information systems contexts and supports the systematic development of conceptual architectures.

A conceptual architecture was systematically developed based on the previously synthesized requirements using a MBSE approach. MBSE provides a formalized way to transform requirements into a rigorous system model and to validate the design at the conceptual level. In fact, MBSE is defined as the formal application of modeling to support system requirements, design, and analysis, along with verification and validation, beginning in the conceptual design phase. Adopting MBSE thus ensured that even at this early stage, the architecture could be checked against stakeholder needs and constraints. The methodology enabled a unified representation of the entire system that made it possible to study interactions among components and agents before implementation. Each requirement was traced to corresponding elements in the model (e.g.~agent roles, interactions, or policies), guaranteeing requirements-to-design traceability and consistency throughout the design process.

The conceptual architecture was captured as a SysML v2 model (the latest OMG Systems Modeling Language standard for systems modeling) leveraging a plain-text modeling workflow in Eclipse Systems Mode. This setup allowed the use of SysMLv2's textual notation to define the system's structure and behavior in a tool-agnostic, version-controlled manner. The choice of SysMLv2 (over SysMLv1 or informal diagrams) is justified by its improved expressiveness and alignment with current MBSE best practices; as an OMG-developed language it provides robust semantics for specifying complex interactions in MAS while remaining an emerging industry standard. In summary, the information system's design was conducted in a model-driven fashion: the SysML~v2 conceptual model served as an executable blueprint of the architecture, facilitating early validation of design decisions against the requirements and providing a solid foundation for subsequent development steps (cf. Madni 2023; OMG 2023).

\section{Literature Review}\label{sec:lit-rev}

\subsection{Operational Excellence}\label{subsec:op-ex}
OpEx originated as a management philosophy in the manufacturing sector, particularly in the automotive industry to optimize quality and efficiency (Lean, Six Sigma, and Total Quality Management). In this classical context, OpEx focused on minimizing defects, eliminating waste, and embedding continuous improvement practices into organizational routines \parencite{juranQuality1999, womackLean2013}. While these roots remain important, they provide only a partial foundation for understanding OpEx in today's IT-driven enterprises, which operate in volatile environments shaped by rapid technological change, regulatory complexity, and global competition.

In IT-driven firms, OpEx is defined less by physical production flows and more by the ability to execute business strategies effectively and efficiently while maintaining innovation and adaptability. A systematic review by \textcite{owoadeSystematic2024} emphasizes that strategic decision-making, leadership, process optimization, and technology integration are the principal drivers of OpEx in IT.~Leaders align strategic goals with day-to-day operations, process optimization ensures efficiency and service reliability, and the integration of emerging technologies---such as cloud computing, automation, and artificial intelligence---enables scalability, agility, and data-driven decision-making. These elements together form the foundation for delivering operational performance in a digital economy.

Recent research points to a structural tension between the stability fostered by OpEx and the adaptability required by organizational agility. According to \textcite{carvalhoOperational2023}, OpEx programs benefit organizations operating in stable contexts by promoting efficiency and rigor, but in volatile environments these same characteristics may reduce responsiveness and even become counterproductive. Agility, in contrast, emphasizes change-readiness and rapid adaptation, qualities increasingly seen as indicators of organizational excellence in globalized and unpredictable markets. The trade-offs are visible: process maturity can limit flexibility, while a focus on speed may undermine quality or compliance. Firms therefore face the challenge of reconciling continuous improvement with adaptability.

However, IT firms also face significant challenges: resistance to change among employees, misalignment between strategy and operational execution, and compliance with evolving regulations.~Externally, competition and technological disruption add further pressures, requiring firms to balance efficiency with innovation. Internally, communication breakdowns and shortages of skilled personnel can limit the adoption of excellence programs. Overcoming these barriers requires strong leadership commitment, alignment of culture with strategic goals, and sustained investment in skills and infrastructure.

From these insights, OpEx in IT can be understood as a dynamic balance: stability through disciplined continuous improvement, complemented by agility to adapt under uncertainty. For mature IT organizations, OpEx provides the governance and process reliability needed to scale, while agility ensures responsiveness to change. For startups, OpEx offers a stabilizing framework to institutionalize quality, upon which agile practices can later be layered. This duality directly informs workflow automation in IT:~automation systems must simultaneously enforce compliance and quality while enabling rapid adaptation and innovation.

OpEx in IT contexts extends beyond efficiency to encompass agility, compliance, and culture. The categories identified provide a structured representation of OpEx in digital settings and serve as a conceptual bridge from the literature review to the RE in Section~\ref{subsec:re-sa}. By grounding requirements in these categories, the thesis ensures that the subsequent multi-agent architecture design directly reflects proven enablers of OpEx in IT firms.

\subsection{Workflow Automation}\label{subsec:workflow-auto}
Building on the need to balance stability and agility in operational processes, workflow automation emerged as a means to systematically coordinate and streamline business workflows. It refers to the use of software systems (workflow management systems, WfMS) to orchestrate tasks, information flows, and decisions along a predefined business process model. According to industry standards, workflow automation entails routing documents, information, or tasks between participants according to procedural rules, with the goal of reducing manual effort and variability in execution \parencite{basuResearch2002}. Early WfMS in the late 20th century were designed to make work more efficient, to integrate heterogeneous applications, and to support end-to-end processes even across organizational boundaries \parencite{stohrWorkflow2001}.

By encoding business procedures into formal process models that are executed by a central \emph{workflow engine}, organizations could enforce consistent process flows, improve speed and accuracy, and embed compliance checks into routine operations. In essence, pre-AI workflow automation provided a structured, deterministic way to implement business processes in software, directly addressing chronic issues like fragmented manual tasks and data silos in pursuit of OpEx \parencite{basuResearch2002}.

One foundational aspect of workflow automation is \textsc{process orchestration}. Orchestration denotes the centralized coordination of tasks and activities according to a defined process logic. In a typical WfMS, a workflow engine enacts the process model, dispatching tasks to the right resources (human or machine) in the correct sequence and enforcing the business rules at each step \parencite{basuResearch2002}. This engine-driven coordination brings predictability and repeatability to workflows: tasks are executed in a fixed, optimized order with minimal ad-hoc variation. By systematically controlling task flow, early workflow systems could eliminate many manual hand-offs and delays, thereby boosting efficiency and consistency in outcomes \parencite{stohrWorkflow2001}. The orchestration approach essentially translated managerial routines into software: for example, an order processing workflow would automatically route an order through credit check, inventory allocation, shipping, and billing steps without needing human coordination at each transition. Such deterministic sequencing was crucial for achieving the quality and reliability targets of OpEx in an era before adaptive AI capabilities.

A closely related design concern is \textsc{integration}. Workflow automation inherently requires linking together diverse people, departments, and IT systems into an end-to-end process. Literature emphasizes that WfMS must integrate heterogeneous application systems and data sources to allow seamless information flow across functions \parencite{stohrWorkflow2001}. For instance, a procurement workflow might connect an ERP inventory module, a supplier's database, and a financial system so that each step can automatically consume and produce the necessary data. This integration extends beyond technical connectivity; it also encompasses coordinating work across organizational boundaries. As e-business initiatives grew in the 1990s and early 2000s, workflows increasingly spanned multiple organizations (suppliers, partners, customers), demanding inter-organizational process integration \parencite{basuResearch2002}. Research in this period identified the need for distributed workflow architectures that could bridge independent systems and companies.~\textcite{georgakopoulosOverview1995} noted that existing workflow tools had limitations in complex environments, calling for infrastructure to handle \enquote{tex}heterogeneous, autonomous, and distributed information systems}. In practice, this led to the development of interoperability standards (e.g., XML-based process definitions, web service interfaces) and process choreography protocols to ensure that a workflow could progress smoothly even when multiple organizations or platforms were involved. Effective integration was thus an essential condition for workflow automation, enabling the end-to-end automation of processes that formerly stopped at organizational or system boundaries.

To manage complexity and change, \textsc{modularity} in workflow design became another important principle. Rather than hard-coding monolithic process flows, architects sought to break workflows into modular components or sub-processes that could be reused and reconfigured as needed. This component-based approach was accelerated by the rise of service-oriented architectures and e-business \enquote{workflow of services} concepts. For example, composite e-services and e-hubs allow organizations to construct complex workflows by composing smaller service modules. A modular workflow architecture improves maintainability: if a business rule changes or a new subprocess is required, one can update or insert a module without redesigning the entire workflow from scratch. Modularity also underpins adaptability. Ideally, a workflow systematizes routine functions but can be adjusted to accommodate new requirements or variations in the process \parencite{basuResearch2002}. In other words, the literature suggests that well-designed workflow automation should combine standardization with flexibility: processes are structured into clear modules for the \enquote{happy path} of routine operations, yet those modules can be reorchestrated or overridden in exceptional cases. This design philosophy reflects an early recognition that no single process model can anticipate all future conditions, so a degree of configurability must be built in.

Despite efforts to introduce flexibility, traditional workflow automation faced notable challenges with \textsc{exception handling}. Exception handling refers to the ability of a system to cope with deviations, errors, or unforeseen scenarios that fall outside the predefined process flow.~\textcite{basuResearch2002} candidly observe that existing WfMS \enquote{tend to fall short whenever workflows have to accommodate exceptions to normal conditions}---i.e., when something unexpected occurs that was not explicitly modeled, the system often cannot resolve it autonomously, forcing human intervention. Typically, designers might anticipate a limited number of exception scenarios and build alternate paths for those (e.g., an approval escalation if a manager is absent). However, if a novel exception arises (say, a new regulatory requirement or an unplanned system outage affecting a step), the rigid workflow cannot handle it, and manual workarounds are needed. This brittleness of early workflows under dynamic conditions was widely acknowledged. Research proposed various approaches to improve exception handling, such as more advanced process metamodels and integration of AI or rule-based decision support to catch and respond to anomalies \parencite{basuResearch2002}. Yet, in the pre-AI era, most workflow automation remained predominantly rule-driven and inflexible outside of predefined contingencies. Exception handling thus stood out as a critical limitation of classical automation approaches, highlighting a gap between the desire for end-to-end automation and the reality of complex, ever-changing business environments.

Another salient theme in the literature is \textsc{workflow governance}---the structures and mechanisms for overseeing automated workflows and aligning them with organizational policies. As companies entrusted core business processes to software, ensuring the correct and intended execution of those processes became vital. Key governance considerations include monitoring, auditing, and controlling workflows. A WfMS typically provides monitoring dashboards and logs so that managers can track the state and performance of process instances (e.g., to identify bottlenecks or errors). It also enforces role-based access control, ensuring that only authorized personnel perform certain tasks or approvals, which is essential for compliance in regulated industries.~\textcite{basuResearch2002} highlight the importance of organizational \enquote{metamodels} and control mechanisms that tie workflows to an enterprise’s structure – for example, defining which organizational roles are responsible for each task and how escalation or overrides should happen under specific conditions. Additionally, governance extends to establishing standards and best practices for workflow design and deployment. Industry coalitions and standards bodies (such as the WFMC in the 1990s) issued reference models and interface standards to promote consistency and interoperability in workflow implementations. In the context of inter-organizational workflows, governance also means agreeing on protocols and service-level commitments between partners so that automated interactions remain trustworthy and transparent. Overall, robust governance in workflow automation ensures not only efficiency but also accountability, security, and compliance. It addresses the managerial and oversight challenges that arise once processes are no longer directly handled by individuals but by software agents following prescribed logic.

From the structured analysis of these sources, pre-AI workflow automation can be summarized by a set of key architectural categories. These represent the dominant design objectives and constraints that had to be addressed in classical workflow systems, and they mirror the strengths as well as the limitations of those systems. The pre-AI workflow automation literature established a foundation of structured, rule-driven process management focused on efficiency, integration, and control. The categories above encapsulate both the core capabilities that made traditional workflow systems valuable and the pain points (like inflexibility in the face of change) that constrained their applicability. These insights provide a structured basis for the requirements derivation in Section~\ref{sec:mod-req} and guide the architectural considerations in later chapters. By grounding the design in these well-understood aspects of workflow automation, the thesis ensures that the proposed multi-agent architecture builds on proven practices while also targeting the gaps. Indeed, many of the limitations noted here---especially around exception handling and adaptiveness---motivate the incorporation of agentic AI elements in the next section, which explores how intelligent agents can augment and transform workflow automation to better achieve OpEx.

\subsection{Agentic Artificial Intelligence}\label{subsec:agentic-ai} 
Agentic AI refers to systems composed of multiple interacting generative agents that autonomously collaborate to achieve complex goals. It represents a shift beyond single AI agents toward orchestrated multi-agent ecosystems, enabled largely by recent advances in the field. Whereas traditional automation (e.g.~rule-based RPA) executes predefined steps, an agentic architecture features \emph{adaptive, goal-directed agents} that can perceive context, make decisions, and act with minimal hard-coded instructions. This idea builds on classic MAS principles of autonomy and social action \parencite{castelfranchiModelling1998,ferberMultiagent1999}, but now agents are augmented with learning and reasoning capabilities from large language models (LLMs).~\textcite{sapkotaAI2026}, highlight this paradigm shift by highlightin the difference between AI agents and \emph{agentic AI}, framing the later as \textit{\enquote{multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy}} in pursuit of flexible problem-solving.

A defining trait of agentic AI is a higher degree of \textsc{autonomy}. Agents can operate without constant human or central control, making and executing decisions in real time. Early MAS research already emphasized agent autonomy---e.g.~agents as entities with independent control over their actions and state. Modern generative agents greatly amplify this autonomy by leveraging LLM-based reasoning to plan multi-step actions toward goals.~For instance, frameworks like AutoGPT \parencite{yangAutoGPT2023} demonstrated that a single LLM-based agent can iteratively break down objectives, choose actions, and adjust based on feedback without human intervention. This autonomy promises agility and decision quality (agents can respond to situational changes or large search spaces beyond rigid scripts), but it also introduces risks. As \textcite{russellResearch2015} caution, each additional decision delegated to an opaque AI agent shifts \enquote{ethical control} away from human operators. In enterprise settings, uncontrolled autonomous decisions might lead to policy violations or unsafe actions \parencite{gauravGovernance2025}. Thus, an architectural challenge is balancing agent freedom with mechanisms to supervise or constrain critical decisions. In practice, this means designing agents with clearly scoped authorities, fail-safes, or escalation paths (e.g.~requiring human confirmation for high-impact actions) to align autonomy with organizational policies.

\textsc{tool-use capability} is another hallmark of agentic AI architectures. Simply put, agents are not limited to their built-in knowledge; they can invoke external tools, APIs, or other services as part of their reasoning loop. This extends an agent's functionality---for example, an AI agent might call a database, run a code snippet, or query web services to gather real-time information. Research shows that augmenting LLM agents with tool integration significantly improves their problem-solving scope and accuracy. Notably, the \emph{ReAct} paradigm interleaves an agents chain-of-thought with tool calls, allowing it to \emph{perceive} (via queries) and \emph{act} (via external operations) iteratively \parencite{yaoReAct2023}. Such designs transform static LLMs into \emph{dynamic cognitive agents} that can \emph{perceive, plan,} and \emph{adapt}, a critical capability for complex, multi-step workflows. For workflow automation, this means an agent can not only parse instructions but also execute parts of a workflow (e.g.~trigger an RPA bot or send an alert) and then reason over the results. Architecturally, enabling tool use requires adding interface layers for the agent to safely interact with enterprise systems (APIs, databases, RPA scripts), along with policies on allowed tools. It's worth noting that tool integration adds orchestration complexity and potential error propagation paths \parencite{sapkotaAI2026}. Therefore, designs often include an \emph{orchestration layer} or planner agent that manages when and how tools are invoked, checks tool outputs, and handles exceptions (e.g.~what if a tool fails or returns unexpected data). In summary, tool-use greatly enhances agent capabilities, but it demands careful architectural planning to manage the added complexity and ensure robust tool-agent interaction. 

Agentic AI systems are inherently multi-agent---they comprise not one but many agents, often with specialized roles, that must \textsc{coordinate} their efforts. This multi-agent approach stems from the insight that complex workflows can be decomposed: instead of one monolithic AI agent trying to do everything, a team of agents can each handle subtasks and then combine results. Such \textsc{specialization} aligns with principles of OpEx (e.g.~division of labor and expertise) and has been shown to improve performance. For example, \textcite{shuEffective2024} report that a collaborative team of LLM-based agents achieved up to 70\% higher success rates on complex tasks compared to a single-agent approach. Architecturally, coordination mechanisms are crucial to harness these gains. Agents need to communicate their intentions, share data/results, and synchronize plans. The literature distinguishes coordination structures along two dimensions: \emph{hierarchy} vs.~\emph{flat} and \emph{centralized} vs~\emph{decentralized} decision-making. In a centralized hierarchical design, a top-level planner/manager agent delegates tasks to subordinate agents and integrates their outputs (akin to a project manager overseeing specialists). This can simplify global coordination and ensure alignment with a single source of truth (the planner's goal), at the cost of a single point of failure or bottleneck. Conversely, decentralized teams use peer-to-peer negotiation or voting; all agents are more equal and collectively decide on task assignments or conflict resolution (drawing on concepts from distributed AI and game theory).~For instance, one recent system had developer agents jointly agree on a solution design without a central boss, mimicking consensus decision-making \parencite{qianChatDev2024}.
Each approach has trade-offs: hierarchical control can be more efficient for well-structured processes, while decentralized collaboration may be more robust to single-agent failure and better for ill-structured problems.~\emph{Inter-agent} communication protocols (what messages agents send and when) are another design facet---simple cases use direct message passing or shared memory, whereas more complex setups might use an event-bus or blackboard architecture for asynchronous communication. Importantly, specialization means each agent can be bounded in scope (e.g.~a compliance checker agent vs.~a data retrieval agent), which helps with scalability: each agent's LLM or reasoning module can operate within a focused context window, and different team members can even use different model types suited to their niche. This modularity and specialization, orchestrated through well-defined coordination logic, is a key architectural strength of agentic AI.~It mirrors how human organizations structure teams for efficiency, and indeed is crucial for aligning multi-agent AI workflows with complex enterprise processes. 

As agent behaviors become more autonomous and distributed, ensuring \textsc{observability} of the system is vital. Observability here means that the internal states, decisions, and actions of agents can be monitored and understood by humans or supervisory systems. Traditional MAS literature often dealt with observability in terms of state visibility (e.g.~in partially observable environments), but in an enterprise context it translates to runtime transparency and traceability of what agents are doing and why. One challenge is that LLM-driven agents reason in natural language (or latent vectors), making their decision process somewhat opaque.~\textcite[cf][p. 29]{sapkotaAI2026} highlight that AI agents \enquote{lack transparency, complicating debugging and trust}, and they advocate for robust logging and auditing pipelines to make agent operations inspectable. In practice, agentic architectures include components to log key events: each prompt an agent generates, each tool API call and its result, each decision or plan the agent commits to, etc. Such audit logs enable post-hoc analysis, error tracing, and explanations---for example, if a workflow failed or a compliance issue occurred, developers can replay the agent interactions to pinpoint the cause. Some frameworks even expose an agent's chain-of-thought (the intermediate reasoning steps) in a controlled way for debugging or compliance review. Beyond logging, observability can be enhanced through dashboarding and alerts: e.g.~real-time monitors that track agent performance metrics or detect anomalies (like an agent taking too long on a task or generating an out-of-bounds output). The end goal is to treat an agentic AI system not as a \enquote{black box} automation, but as an observable workflow that operations teams can supervise akin to any critical IT system. This also ties into explainability: by capturing the rationale behind decisions (even if only in approximate form, such as storing the intermediate reasoning text), the system can later provide explanations for its actions, which is invaluable for trust and for continuous improvement. Overall, the literature suggests that designing for transparency---instrumenting agents with logging, and perhaps even designing agents to self-report their status---is a best practice to ensure agentic AI doesn't become an inscrutable tangle of automations. High observability supports OpEx principles by enabling \emph{traceability, accountability,} and \emph{faster incident response} when something goes wrong.

Finally, a recurrent theme is the need for strong \textsc{governance} mechanisms in agentic AI architectures to ensure alignment with rules, ethics, and organizational policies. By their nature, autonomous agents may produce unexpected or undesired outcomes---a risk amplified in multi-agent settings where interactions are complex and no single agent has full oversight. Without proper governance, an agentic system could easily violate compliance requirements or strategic constraints, undermining OpEx goals (e.g.~a well-intentioned agent might inadvertently expose sensitive data or execute an unauthorized transaction). In fact, \textcite{gauravGovernance2025} warn that the \enquote{absence of scalable, decoupled governance remains a structural liability} in today's agentic AI ecosystems. To address this, researchers are exploring policy-enforcement layers that sit between the agents and the outside world. One such approach is Governance-as-a-Service (GaaS), a framework that intercepts agent actions at runtime and checks them against explicit rules or constraints. Rather than trusting each agent to self-regulate, an external governance layer can block or redirect high-risk actions, log rule violations, and even adapt penalties or restrictions on agents that exhibit misbehavior over time. This effectively creates an oversight controller for the MAS---analogous to a \enquote{compliance officer} in a human organization---that ensures no single agent can compromise the system's integrity. Key design elements include declarative policy rules (defining allowable vs.~disallowed outputs or tool uses), a mechanism to monitor all agent outputs (to flag violations), and possibly a trust score or reputation model to quantify an agent's reliability based on past behavior. Beyond automated enforcement, governance also encompasses human oversight: for example, requiring human approval for certain agent decisions (human-in-the-loop checkpoints) or having a fallback where a human operator can intervene if the agents encounter an ambiguous ethical situation. In classical MAS research, analogous concepts existed like normative agents and electronic institutions that enforce \enquote{rules of engagement} among agents; the new twist is that with LLM-based agents we must often treat the models as black boxes, so governance can't be injected into their internal logic easily and must surround them instead. The overarching recommendation is that any architecture for generative multi-agent workflows should bake in governance from the start---not as an afterthought---to manage risk. As one author succinctly put it, such a system \enquote{does not teach agents ethics; it enforces them}. This governance emphasis aligns tightly with OpEx goals of \emph{compliance, risk management,} and \emph{trustworthiness.}

In summary, the literature portrays agentic AI as a powerful paradigm for workflow automation that, if well-designed, can dramatically enhance agility, adaptability, and decision quality in operations. By combining autonomous, tool-using agents into coordinated architectures, organizations can automate complex processes that previously required human judgment. At the same time, achieving sustainable excellence with such systems demands careful attention to transparency and control: architects must ensure agents remain observable and governable to uphold compliance and reliability standards. These insights set the stage for the next section of this thesis, which will integrate OpEx principles, workflow automation requirements, and agentic AI capabilities into a unified reference architecture. The themes of autonomy, coordination, and governance identified here directly inform the design choices and requirements elaborated in the subsequent chapters.

\section{Modeling the Requirements}\label{sec:mod-req}
The literature review identified recurring design concerns across operational excellence, workflow automation, and agentic AI.~Synthesizing these insights yields \emph{elicitation lists} which represent the initial outcome of requirements documentation based on systematically coded sources. A subsequent \emph{clustering} step consolidated overlapping items into a unified set of requirement candidates.

Each candidate was then reformulated into an atomic, unambiguous, and verifiable “shall” statement, following the best-practice formulation rules of \textcite{glinzHandbook2020}. The final requirements were organized into \emph{functional requirements}, \emph{quality requirements}, and \emph{constraints}, in line with the Glinz taxonomy.

Requirements were then represented in SysML~v2 as dedicated requirement elements, with their textual statements captured in the description field. Trace links connect each requirement to its source in the elicitation lists and to the architecture elements that \emph{«satisfy»} it. This model-based representation ensures that design decisions remain traceable to validated needs and that requirement coverage can later be verified systematically. \\

\subsection{Requirements Clustering \& Consolidation}

\noindent\textsc{O --- operational excellence}
\begin{enumerate}
  \item \textsc{adaptability and agility} --- processes must remain reconfigurable in response to volatile conditions, ensuring resilience in dynamic environments.
  \item \textsc{compliance and risk management} --- regulatory adherence and transparency must be embedded into workflows to minimize compliance risks.
  \item \textsc{decision quality} --- automation should enable data-driven, timely, and well-informed decisions rather than simply increasing speed.
  \item \textsc{efficiency and continuous improvement} --- workflows should reduce manual effort, eliminate waste, and institutionalize iterative refinements.
  \item \textsc{customer-centricity} --- operations must align with user needs and service-level commitments to sustain value delivery.
  \item \textsc{user empowerment and culture} --- systems should support collaboration, transparency, and employee engagement in improvement processes.
  \item \textsc{technology integration and scalability} --- architectures must accommodate automation, AI, and cloud services to enable sustainable innovation.
\end{enumerate}

\noindent\textsc{W --- workflow automation}
\begin{enumerate}
  \item \textsc{process orchestration} --- workflow engines must enforce task sequences and business rules to guarantee reliable execution.
  \item \textsc{integration and interoperability} --- automation must seamlessly connect heterogeneous applications, data sources, and organizational boundaries.
  \item \textsc{modularity and reusability} --- workflows should be composed of modular tasks or subprocesses that can be reused and reconfigured with minimal effort.
  \item \textsc{exception handling and flexibility} --- systems must detect, manage, and escalate deviations rather than failing in unforeseen scenarios.
  \item \textsc{workflow governance} --- monitoring, audit trails, and role-based controls must ensure accountability and compliance throughout automated processes.
\end{enumerate}

\noindent\textsc{A --- agentic ai}
\begin{enumerate}
  \item \textsc{autonomy in decision-making} --- agents should operate independently within clearly scoped authority to enhance agility while managing risks.
  \item \textsc{tool use and integration} --- agents must invoke external tools, APIs, or services reliably, requiring robust interfaces and safeguards.
  \item \textsc{coordination and specialization} --- multi-agent systems should divide labor through explicit roles and structured coordination mechanisms.
  \item \textsc{observability and transparency} --- all agent actions and decisions must be logged and explainable to support trust, debugging, and compliance.
  \item \textsc{governance and compliance} --- oversight mechanisms, including policy enforcement layers and human-in-the-loop checkpoints, are essential to align agent behavior with organizational and ethical standards.
\end{enumerate}

Together, these elicitation lists represent the distilled outcome of the literature review. In RE terminology, they correspond to the \emph{documentation} of raw requirements prior to formal specification.~In the next step, a \emph{clustering} analysis was conducted to identify similarities and resolve redundancy across the three lists (e.g., governance appears in both workflow automation and agentic AI). This step corresponds to the \emph{analysis phase} in \textcite{herrmannGrundlagen2022} RE cycle, where elicited items are consolidated and normalized before specification.

% Here is where the second general clustering takes place. Merging the three initial lists into one. A simple table can be done referring to the requirements by their ID.

\subsection{Requirement Reformulation \& Classification}\label{subsec:req-clas}
% once the requirements are clustered into one list, they have to be processed to comply with RE best practices.
Following \textcite{glinzHandbook2020}, each requirement is formulated as a single, unambiguous “shall” statement that is necessary, atomic, verifiable, and consistent. Requirements are classified into functional, quality, and constraint types. In SysML v2, requirements are modeled as dedicated requirement elements with their textual content captured in the description field, and their relationships (e.g. «satisfy», «verify») expressed through model links. Rationale and verification considerations are documented narratively in the requirements analysis and traceability sections, while the requirements themselves remain concise textual statements embedded in the SysML v2 model.

% Once done this, the requirements are classified by type: functional, quality, constraint in each subsection. 
Functional requirements specify externally visible system behaviors, separated from quality attributes and constraints. They were elicited from the categorized insights of the literature review, documented in “shall” form, and prepared for trace links in the SysML requirements model. \\

\noindent \textsc{FR-01 process orchestration engine} \\
\indent \emph{Statement—} The system shall execute workflow models by dispatching tasks to human or software actors according to model control flow and business rules. \\
\indent \emph{Rationale—} Centralized orchestration ensures reliable, repeatable execution. \\

\noindent \textsc{FR-02 task assignment \& role routing} \\
\indent \emph{Statement—} The system shall route tasks based on roles, skills, and authorization. \\
\indent \emph{Rationale—} Role- and skill-aware routing aligns work with organizational responsibilities. \\

\noindent \textsc{FR-03 task reassignment and escalation} \\
\indent \emph{Statement—} The system shall ensure task continuity by supporting reassignment to eligible actors and enforcing time-based escalation policies. \\
\indent \emph{Rationale—} Continuity mechanisms prevent stalls and maintain service levels. \\

\noindent \textsc{FR-04 model ingestion} \\
\indent \emph{Statement—} The system shall ingest workflow or process definitions in a machine-readable format and make them available for execution and versioning. \\
\indent \emph{Rationale—} Importable models enable governance, repeatability, and controlled change. \\

\noindent \textsc{FR-05 enterprise integration} \\
\indent \emph{Statement—} The system shall provide connectors to interact with external applications, data sources, and services as workflow steps. \\
\indent \emph{Rationale—} Interoperability enables end-to-end automation across heterogeneous systems. \\

\noindent \textsc{FR-06 agent tool use} \\
\indent \emph{Statement—} The system shall allow agent components to invoke approved external tools or APIs with controlled inputs/outputs and capture results for downstream steps. \\
\indent \emph{Rationale—} Tool use turns agents into capable actors while containing risk. \\

\noindent \textsc{FR-07 inter-agent coordination} \\
\indent \emph{Statement—} The system shall support coordination patterns (e.g., hierarchical, peer-to-peer, brokered) among multiple agents executing tasks. \\
\indent \emph{Rationale—} Structured coordination prevents conflicts and enables distributed problem solving. \\

\noindent \textsc{FR-08 exception handling} \\
\indent \emph{Statement—} The system shall detect execution errors and deviations, support compensating actions and escalation, and enable resumable recovery. \\
\indent \emph{Rationale—} Robust exception handling preserves correctness and availability. \\

\noindent \textsc{FR-09 workflow governance} \\
\indent \emph{Statement—} The system shall enforce access control, maintain audit trails, and ensure compliance with organizational policies during workflow execution. \\
\indent \emph{Rationale—} Governance guarantees accountability and regulatory conformity. \\

\noindent \textsc{FR-10 autonomy in decision-making} \\
\indent \emph{Statement—} The system shall enable agents to make decisions within explicitly scoped authority without requiring constant human input. \\
\indent \emph{Rationale—} Scoped autonomy improves responsiveness while containing risk. \\

\noindent \textsc{FR-11 observability \& transparency} \\
\indent \emph{Statement—} The system shall log agent decisions, actions, and tool invocations in a human-readable and queryable format. \\
\indent \emph{Rationale—} Transparent logs support trust, auditing, and debugging. \\

\noindent \textsc{FR-12 human-in-the-loop control \& runtime views} \\
\indent \emph{Statement—} The system shall provide configurable human approval/override points and runtime execution views, including replay capability. \\
\indent \emph{Rationale—} Human oversight and operational views enable intervention and diagnosis. \\

\noindent \textsc{FR-13 policy \& compliance enforcement} \\
\indent \emph{Statement—} The system shall validate agent actions against declarative policies and block or redirect disallowed behaviors. \\
\indent \emph{Rationale—} Policy enforcement prevents rule violations and unsafe behavior. \\

\noindent \textsc{FR-14 risk-based routing} \\
\indent \emph{Statement—} The system shall evaluate risk signals (e.g., anomalies, policy matches) and prefer safer modeled paths without overriding explicit policy decisions. \\
\indent \emph{Rationale—} Risk-based routing provides adaptive safeguards while preserving compliance. \\

\noindent \textsc{FR-15 configuration, scalability \& explainability} \\
\indent \emph{Statement—} The system shall maintain versions of process models, policies, agent roles, and connectors; support controlled rollout and rollback of configurations; distribute workflows and agents across execution environments; and provide justifications for agent decisions with references to inputs, rules, and tools used. \\
\indent \emph{Rationale—} Versioning, scalability, and explainability together ensure safe evolution, resilience, and trust in system operation. \\

\noindent\emph{Verification approach.} Each functional requirement is verifiable by inspection (model presence), analysis (policy/model correctness), or test (execution against acceptance scenarios). The SysML model (Appendix) will provide trace links from FR-IDs to architecture elements (agents, gateways, connectors) and validation scenarios.

\subsection{Model-Based Requirements Representation}\label{subsec:req-model}

\section{Modeling the Architecture}\label{sec:mod-mas}
\begin{listing}[h]
    \caption{Excerpt of the Requirements model}
    \inputminted[firstline=1,lastline=25]{text}{ressources/models/requirements.sysml}
\end{listing}
\subsection{[placeholder]Modeling the Agents}\label{subsec:mod-agents}
\subsection{[placeholder]Modeling the Architecture}\label{subsec:mod-arch}
\subsection{[placeholder]Modeling the Interactions}\label{subsec:mod-interactions}

\section{Discussion: Applicability Criteria}\label{sec:discussion}
    
\section{Conclusion}\label{sec:conclussion}
Future work should extend this conceptual design into practical evaluation and implementation. In particular, empirical validation of the architecture in industry settings, tool-supported instantiation in SysML, and comparative studies against traditional workflow automation would provide valuable evidence of its applicability and impact. Further, integrating additional agentic AI capabilities such as autonomous negotiation or explainability could enhance both usability and compliance assurance.
\clearpage