\section{Introduction}\label{sec:intro}
Organizations across industries continue to face persistent challenges in achieving operational excellence (OpEx). Fragmented processes, manual interventions, and inconsistent data quality undermine efficiency and decision-making. Legacy workflows and siloed systems exacerbate these inefficiencies, while traditional automation approaches often lack the adaptability needed in dynamic business environments. For companies, this translates into slower response times, higher compliance risks, and limited scalability—issues that directly threaten competitiveness.

Generative artificial intelligence (GenAI) opens new possibilities to extend automation beyond deterministic scripts, enabling adaptive, tool-using agents that support governance, decision quality, and agility. However, despite this potential, organizations and the academic literature alike lack structured strategies and conceptual frameworks for embedding such agentic capabilities into operational workflows in a scalable, value-driven way. This gap motivates the present research.

In this context, while GenAI provides agentic capabilities, multi-agent systems can serve as a reference architecture for integrating GenAI into enterprise workflow automation. The central research question is:

\vspace{0.5\baselineskip}
\emph{How can a multi-agent architecture be designed to integrate GenAI capabilities into workflow automation, in order to enhance agility, compliance, \& decision quality to achieve OpEx?}
\vspace{0.5\baselineskip}

To answer this question, the study addresses the following sub-questions:
\begin{itemize}
    \item \emph{What are the strengths \& limitations of GenAI in the context of workflow automation using a multi-agent architecture?}
    \item \emph{Which design requirements \& agent roles are necessary to align a multi-agent architecture with the goals of OpEx?}
    \item \emph{Under which conditions is deploying a generative multi-agent architecture justified over traditional automation approaches?}
\end{itemize}

Methodologically, the thesis applies Design Science Research (DSR) to develop a conceptual reference architecture. The approach synthesizes requirements from academic literature and OpEx principles, models agent roles and interactions, and derives applicability conditions for real-world deployment.

The core contribution of this work is a conceptual design of a multi-agent system that leverages GenAI to support OpEx in enterprise workflows. Specifically, it delivers:
\begin{enumerate}
    \item A structured synthesis of system requirements derived from academic literature and OpEx principles.
    \item A conceptual architecture detailing agent roles, interactions, and integration points
    \item A set of applicability conditions and design considerations to guide future deployment and evaluation of generative multi-agent architectures in practice.
\end{enumerate}

The scope is limited to conceptual design; formal evaluation and technical implementation are proposed as future work. The approach remains industry-agnostic but draws illustrative examples from the financial services sector, given its regulatory complexity and reliance on legacy systems.

The thesis is structured as follows: Section 2 outlines the research methodology, including DSR and supporting methods. Section 3 presents a literature review on OpEx, automation paradigms, and multi-agent systems. Section 4 develops applicability conditions and use case illustrations. Section 5 introduces the conceptual architecture design, and Section 6 concludes with reflections and directions for future research.

\section{Methodology}\label{sec:method}
This thesis applies \emph{design science research methodology} to create a conceptual artifact---\emph{a multi-agent architecture for workflow automation}. Practically, the approach unfolded in three steps: (1) reviewing the literature on OpEx, workflow automation, and agentic AI; (2) deriving and structuring requirements from literature and case material into a requirements model; and (3) designing a conceptual system architecture using System Modeling Language (SysML).

Supporting methods included Mayring-style qualitative content analysis (QCA) for the review, requirements engineering (RE) and systems analysis for the requirements model, and model-based systems engineering (MBSE) to structure the architecture and ensure requirement-to-design traceability. Within DSR, the work focuses on problem identification, objective definition, and conceptual design, while instantiation/demonstration and formal evaluation are out of scope given the bachelor-thesis format and resource constraints. This scoping maintains methodological rigor while keeping the contribution focused: a well-argued reference architecture ready for subsequent implementation and empirical evaluation.

\subsection{Qualitative Content Analysis}\label{subsec:qca}
To ensure a structured literature review, this thesis employed qualitative content analysis following \textcite{mayringQualitative2022}. 
QCA offers a transparent, rule-based procedure for synthesizing knowledge from textual sources while retaining interpretative depth. 
In this work it supports the DSR process \parencite{peffersDesign2007} within the \emph{problem identification and motivation} phase, 
where the aim is to understand the state of the problem domain and justify the value of a solution.

Following Mayring, the analytical framework was defined prior to coding:
\begin{itemize}
    \item \textit{Analysis unit}: the overall literature corpus addressing OpEx, workflow automation, and agentic AI.
    \item \textit{Context unit}: individual publications (books, peer-reviewed articles, standards, industry reports, conference transcripts, and case studies).
    \item \textit{Coding unit}: discrete statements or conceptual claims relevant to the intersection of OpEx, automation paradigms, and AI-based multi-agent systems.
\end{itemize}

A \emph{mixed deductive--inductive} approach was used. Deductive categories were derived from established theory, including OpEx dimensions---adaptability, compliance, decision quality---and prior automation frameworks---robot process automation (RPA), intelligent process automation (IPA). 
Inductive categories were generated from the material itself, capturing emerging issues such as ``guardrails,'' ``observability,'' and ``traceability'' in agentic AI systems. 
Coding followed Mayring's rule-governed categorization to ensure consistency and avoid arbitrary interpretation.

The resulting categories served two functions:
\begin{itemize}
    \item \textit{Problem representation}: categories structured how the research problem was represented, aligning with \textcite{hevnerDesign2004}, who emphasize that effective constructs are essential to problem framing.
    \item \textit{Derivation of objectives}: categories were transformed into metarequirements that guided the definition of solution objectives in Activity~2 of the DSR methodology \parencite{peffersDesign2007}.
\end{itemize}

The review was conducted by systematically coding the literature across the three pillars (OpEx, workflow automation, multi-agent systems). 
For example, claims such as ``RPA is brittle under interface changes'' were coded under the deductive category \emph{limitations of RPA}, 
while repeated references to audit trails and logging practices were inductively grouped under \emph{traceability}. 
For each publication, relevant statements were assigned to categories using predefined coding rules. 
The resulting category set both structures Section~\ref{sec:lit-rev} and forms the basis for the requirements engineering in Section~\ref{subsec:re-sa}. Additional categories such as efficiency, customer-centricity, and user empowerment emerged inductively during coding; these are elaborated in Section~\ref{subsec:op-ex}.

\subsection{Requirements Engineering \& System Analysis}\label{subsec:re-sa}

\subsection{Model-Based Systems Engineering}\label{subsec:mbse}

\section{Literature Review}\label{sec:lit-rev}

\subsection{Operational Excellence}\label{subsec:op-ex}
As introduced in Section~\ref{subsec:qca}, the categories derived from the QCA frame this review of OpEx. OpEx originated as a management philosophy in the manufacturing sector, particularly in the automotive industry, where it drew upon Lean, Six Sigma, and Total Quality Management to optimize quality and efficiency \parencite{juranQuality1999, womackLean2013}. In this classical context, OpEx focused on minimizing defects, eliminating waste, and embedding continuous improvement practices into organizational routines. While these roots remain important, they provide only a partial foundation for understanding OpEx in today's IT-driven enterprises, which operate in volatile environments shaped by rapid technological change, regulatory complexity, and global competition.

Mayring's QCA approach allowed the definition of clear analytical units (e.g. definitions and principles of OpEx from each source), apply deductive codes from established theory (e.g. lean principles, quality management frameworks), and derive inductive codes emerging specifically in IT settings and automation contexts. The main categories extracted support and inform the design of requirements in this DSR project.

In IT-driven firms, OpEx is defined less by physical production flows and more by the ability to execute business strategies effectively and efficiently while maintaining innovation and adaptability. A systematic review by \textcite{owoadeSystematic2024} emphasizes that leadership, process optimization, and technology integration are the principal drivers of operational excellence in IT. Leaders align strategic goals with day-to-day operations, process optimization ensures efficiency and service reliability, and the integration of emerging technologies---such as cloud computing, automation, and artificial intelligence---enables scalability, agility, and data-driven decision-making. These elements together form the foundation for delivering operational performance in a digital economy.

Recent research points to a structural tension between the stability fostered by OpEx and the adaptability required by organizational agility. According to \textcite{carvalhoOperational2023}, OpEx programs benefit organizations operating in stable contexts by promoting efficiency and rigor, but in volatile environments these same characteristics may reduce responsiveness and even become counterproductive. Agility, in contrast, emphasizes change-readiness and rapid adaptation, qualities increasingly seen as indicators of organizational excellence in globalized and unpredictable markets. The trade-offs are visible: process maturity can limit flexibility, while a focus on speed may undermine quality or compliance. Firms therefore face the challenge of reconciling continuous improvement with adaptability. In this thesis, agility is therefore treated as a complementary logic that organizations must balance with OpEx, rather than as a dimension within OpEx itself.

Organizational culture plays a central role in balancing these competing logics. Culture aligns the discipline of OpEx with the flexibility of agility by embedding values of learning, transparency, and collaboration across teams. Mature organizations can extend their well-institutionalized OpEx programs by incorporating agile practices, thereby sustaining quality while adapting to new conditions. Conversely, young organizations often adopt OpEx precisely to stabilize and professionalize their processes before layering agility on top. High levels of quality---a traditional hallmark of excellence---also correlate positively with agility, as quality practices create the reliability necessary for rapid iteration in volatile markets. Culture therefore acts as the mediator that makes simultaneous pursuit of excellence and agility possible.

OpEx in IT firms is not only an abstract philosophy but a set of concrete practices. The review by \textcite{owoadeSystematic2024} identifies strategic business administration practices such as corporate governance, transformational leadership, process optimization (via Lean, Six Sigma, and TQM), and technology integration as critical enablers. However, IT firms also face significant challenges: resistance to change among employees, misalignment between strategy and operational execution, and compliance with evolving regulations such as GDPR. Externally, competition and technological disruption add further pressures, requiring firms to balance efficiency with innovation. Internally, communication breakdowns and shortages of skilled personnel can limit the adoption of excellence programs. Overcoming these barriers requires strong leadership commitment, alignment of culture with strategic goals, and sustained investment in skills and infrastructure.

From these insights, operational excellence in IT can be understood as a dynamic balance: stability through disciplined continuous improvement, complemented by agility to adapt under uncertainty. For mature IT organizations, OpEx provides the governance and process reliability needed to scale, while agility ensures responsiveness to change. For startups, OpEx offers a stabilizing framework to institutionalize quality, upon which agile practices can later be layered. This duality directly informs workflow automation in IT: automation systems must simultaneously enforce compliance and quality while enabling rapid adaptation and innovation.

Applying Mayring’s QCA approach, the analysis of OpEx literature yields the following categories relevant to IT-driven workflow automation:
\begin{itemize}
    \item \textbf{Adaptability and Agility:} the capacity to reconfigure processes in response to volatility.
    \item \textbf{Compliance and Risk Management:} embedding regulatory adherence and transparency into workflows.
    \item \textbf{Decision Quality:} fostering data-driven and timely decision-making through analytics.
    \item \textbf{Efficiency and Continuous Improvement:} reducing waste, automating repetitive tasks, and institutionalizing iterative refinements.
    \item \textbf{Customer-Centricity:} aligning operations with user needs and service-level commitments.
    \item \textbf{User Empowerment and Culture:} supporting collaboration, transparency, and employee engagement in improvement processes.
    \item \textbf{Technology Integration and Scalability:} ensuring architectures can incorporate automation, AI, and cloud services to sustain innovation.
\end{itemize}

In sum, operational excellence in IT contexts extends beyond efficiency to encompass agility, compliance, and culture. The categories identified provide a structured representation of OpEx in digital settings and serve as a conceptual bridge from the literature review to the requirements engineering in Section~\ref{subsec:re-sa}. By grounding requirements in these categories, the thesis ensures that the subsequent multi-agent architecture design directly reflects proven enablers of operational excellence in IT firms.

\subsection{Workflow Automation}\label{subsec:workflow-auto}
Building on the need to balance stability and agility in operational processes, \textit{workflow automation} emerged as a means to systematically coordinate and streamline business workflows. It refers to the use of software systems (workflow management systems, WfMS) to orchestrate tasks, information flows, and decisions along a predefined business process model. According to industry standards, workflow automation entails routing documents, information, or tasks between participants according to procedural rules, with the goal of reducing manual effort and variability in execution \citep{Basu2002}. Early WfMS in the late 20th century were designed to make work more efficient, to integrate heterogeneous applications, and to support end-to-end processes even across organizational boundaries \citep{Stohr2001}. By encoding business procedures into formal process models that are executed by a central \textit{workflow engine} \citep{Basu2002}, organizations could enforce consistent process flows, improve speed and accuracy, and embed compliance checks into routine operations. In essence, pre-AI workflow automation provided a structured, deterministic way to implement business processes in software, directly addressing chronic issues like fragmented manual tasks and data silos in pursuit of operational excellence.

Applying Mayring’s qualitative content analysis (QCA) methodology (cf. Mayring 2022), the literature on classical workflow automation was examined to identify key architectural themes. Deductive codes were derived from established workflow management theory and frameworks (e.g., the Workflow Management Coalition reference model, business process management principles), which highlighted expected elements such as process modeling, integration, and performance control. Inductive codes, in turn, emerged from recurring challenges noted in the sources—issues like handling unexpected exceptions, enabling cross-company processes, and ensuring proper governance of automated workflows. Through this structured analysis, several principal design concerns were distilled, reflecting how pre-AI workflow automation was conceptualized and the requirements it had to fulfill. The following discusses these core themes, which will later inform the multi-agent architecture design.

One foundational aspect of workflow automation is **process orchestration**. Orchestration denotes the centralized coordination of tasks and activities according to a defined process logic. In a typical WfMS, a workflow engine enacts the process model, dispatching tasks to the right resources (human or machine) in the correct sequence and enforcing the business rules at each step \citep{Basu2002}. This engine-driven coordination brings predictability and repeatability to workflows: tasks are executed in a fixed, optimized order with minimal ad-hoc variation. By systematically controlling task flow, early workflow systems could eliminate many manual hand-offs and delays, thereby boosting efficiency and consistency in outcomes \citep{Stohr2001}. The orchestration approach essentially translated managerial routines into software: for example, an order processing workflow would automatically route an order through credit check, inventory allocation, shipping, and billing steps without needing human coordination at each transition. Such deterministic sequencing was crucial for achieving the quality and reliability targets of operational excellence in an era before adaptive AI capabilities.

A closely related design concern is **integration**. Workflow automation inherently requires linking together diverse people, departments, and IT systems into an end-to-end process. Literature emphasizes that WfMS must integrate heterogeneous application systems and data sources to allow seamless information flow across functions \citep{Stohr2001}. For instance, a procurement workflow might connect an ERP inventory module, a supplier’s database, and a financial system so that each step can automatically consume and produce the necessary data. This integration extends beyond technical connectivity; it also encompasses coordinating work across organizational boundaries. As e-business initiatives grew in the 1990s and early 2000s, workflows increasingly spanned multiple organizations (suppliers, partners, customers), demanding inter-organizational process integration \citep{Basu2002}. Research in this period identified the need for distributed workflow architectures that could bridge independent systems and companies. Georgakopoulos et al. (1995) noted that existing workflow tools had limitations in complex environments, calling for infrastructure to handle “heterogeneous, autonomous, and distributed information systems” \citep{Georgakopoulos1995}. In practice, this led to the development of interoperability standards (e.g., XML-based process definitions, web service interfaces) and process choreography protocols to ensure that a workflow could progress smoothly even when multiple organizations or platforms were involved. Effective integration was thus a sine qua non for workflow automation, enabling the end-to-end automation of processes that formerly stopped at organizational or system boundaries.

To manage complexity and change, **modularity** in workflow design became another important principle. Rather than hard-coding monolithic process flows, architects sought to break workflows into modular components or sub-processes that could be reused and reconfigured as needed. This component-based approach was accelerated by the rise of service-oriented architectures and e-business “workflow of services” concepts \citep{Basu2002}. For example, Basu and Kumar (2002) describe how composite e-services and e-hubs allow organizations to construct complex workflows by composing smaller service modules. A modular workflow architecture improves maintainability: if a business rule changes or a new subprocess is required, one can update or insert a module without redesigning the entire workflow from scratch. Modularity also underpins adaptability. Ideally, a workflow systematizes routine functions but can be adjusted to accommodate new requirements or variations in the process \citep{Basu2002}. In other words, the literature suggests that well-designed workflow automation should combine standardization with flexibility: processes are structured into clear modules for the “happy path” of routine operations, yet those modules can be reorchestrated or overridden in exceptional cases. This design philosophy reflects an early recognition that no single process model can anticipate all future conditions, so a degree of configurability must be built in.

Despite efforts to introduce flexibility, traditional workflow automation faced notable challenges with **exception handling**. Exception handling refers to the ability of a system to cope with deviations, errors, or unforeseen scenarios that fall outside the predefined process flow. Basu and Kumar (2002) candidly observe that existing WfMS “tend to fall short whenever workflows have to accommodate exceptions to normal conditions” – i.e., when something unexpected occurs that was not explicitly modeled, the system often cannot resolve it autonomously, forcing human intervention. Typically, designers might anticipate a limited number of exception scenarios and build alternate paths for those (e.g., an approval escalation if a manager is absent). However, if a novel exception arises (say, a new regulatory requirement or an unplanned system outage affecting a step), the rigid workflow cannot handle it, and manual workarounds are needed \citep{Basu2002}. This brittleness of early workflows under dynamic conditions was widely acknowledged. Research proposed various approaches to improve exception handling, such as more advanced process metamodels and integration of AI or rule-based decision support to catch and respond to anomalies \citep{Basu2002}. Yet, in the pre-AI era, most workflow automation remained predominantly rule-driven and inflexible outside of predefined contingencies. Exception handling thus stood out as a critical limitation of classical automation approaches, highlighting a gap between the desire for end-to-end automation and the reality of complex, ever-changing business environments.

Another salient theme in the literature is **workflow governance** – the structures and mechanisms for overseeing automated workflows and aligning them with organizational policies. As companies entrusted core business processes to software, ensuring the correct and intended execution of those processes became vital. Key governance considerations include monitoring, auditing, and controlling workflows. A WfMS typically provides monitoring dashboards and logs so that managers can track the state and performance of process instances (e.g., to identify bottlenecks or errors). It also enforces role-based access control, ensuring that only authorized personnel perform certain tasks or approvals, which is essential for compliance in regulated industries. Basu and Kumar (2002) highlight the importance of organizational “metamodels” and control mechanisms that tie workflows to an enterprise’s structure – for example, defining which organizational roles are responsible for each task and how escalation or overrides should happen under specific conditions. Additionally, governance extends to establishing standards and best practices for workflow design and deployment. Industry coalitions and standards bodies (such as the WFMC in the 1990s) issued reference models and interface standards to promote consistency and interoperability in workflow implementations. In the context of inter-organizational workflows, governance also means agreeing on protocols and service-level commitments between partners so that automated interactions remain trustworthy and transparent. Overall, robust governance in workflow automation ensures not only efficiency but also accountability, security, and compliance. It addresses the managerial and oversight challenges that arise once processes are no longer directly handled by individuals but by software agents following prescribed logic.

From the structured analysis of these sources, pre-AI workflow automation can be summarized by a set of key architectural categories. These represent the dominant design objectives and constraints that had to be addressed in classical workflow systems, and they mirror the strengths as well as the limitations of those systems. The QCA-driven review for this thesis distilled the following categories (both deductively and inductively derived) as particularly relevant:

\begin{itemize}
  \item \textbf{Process Orchestration and Coordination:} Centralized control of process execution through a workflow engine, which dispatches tasks and enforces business rules to ensure activities occur in the correct sequence.
  \item \textbf{Modularity and Reusability:} Composition of workflows from modular tasks or sub-processes that can be reused and reconfigured, allowing the process design to be adapted or extended with minimal effort.
  \item \textbf{Exception Handling and Flexibility:} Mechanisms to detect and manage deviations or unexpected situations in a workflow, enabling the system to handle errors or novel scenarios (or escalate them appropriately) rather than simply failing.
  \item \textbf{Integration and Interoperability:} Seamless linking of diverse applications, data sources, and organizational units into a unified process flow, often via standardized interfaces or protocols, so that automation spans across technological and organizational boundaries.
  \item \textbf{Governance and Compliance:} Oversight and management of workflows through monitoring, audit trails, and role-based controls, ensuring that automated processes remain aligned with business policies, performance targets, and regulatory requirements.
\end{itemize}

In sum, the pre-AI workflow automation literature established a foundation of structured, rule-driven process management focused on efficiency, integration, and control. The categories above encapsulate both the core capabilities that made traditional workflow systems valuable and the pain points (like inflexibility in the face of change) that constrained their applicability. These insights provide a structured basis for the requirements derivation in Section~2.2 and guide the architectural considerations in later chapters. By grounding the design in these well-understood aspects of workflow automation, the thesis ensures that the proposed multi-agent architecture builds on proven practices while also targeting the gaps. Indeed, many of the limitations noted here – especially around exception handling and adaptiveness – motivate the incorporation of agentic AI elements in the next section (3.3), which explores how intelligent agents can augment and transform workflow automation to better achieve operational excellence.
